{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1dc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('dataset_full.parquet')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(f\"Structure du dataset : {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3dc846",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63541ec1",
   "metadata": {},
   "source": [
    "##### Drop des colonnes inutiles au mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. NETTOYAGE G√âOGRAPHIQUE & RISQUE D√âPARTEMENTAL\n",
    "\n",
    "df['Code du d√©partement de l\\'√©tablissement'] = df['Code du d√©partement de l\\'√©tablissement'].astype(str).str.zfill(2)\n",
    "\n",
    "dep_risk_map = df.groupby(\"Code du d√©partement de l'√©tablissement\")[\"fermeture\"].mean()\n",
    "df['risque_departemental'] = df['Code du d√©partement de l\\'√©tablissement'].map(dep_risk_map)\n",
    "\n",
    "# 2. TRAITEMENT DES TYPES (Cat√©gories et Nullables)\n",
    "\n",
    "df['Cat√©gorie juridique de l\\'unit√© l√©gale'] = df['Cat√©gorie juridique de l\\'unit√© l√©gale'].astype(str)\n",
    "df['age_estime'] = df['age_estime'].astype(float)\n",
    "df['Tranche_effectif_num'] = df['Tranche_effectif_num'].fillna(0).astype(float)\n",
    "\n",
    "# 3. ENCODAGE DES VARIABLES\n",
    "\n",
    "df['is_ess'] = df['Economie sociale et solidaire unit√© l√©gale'].map({'O': 1, 'N': 0}).fillna(0)\n",
    "\n",
    "df_final = pd.get_dummies(\n",
    "    df, \n",
    "    columns=['libelle_section_ape', 'Cat√©gorie juridique de l\\'unit√© l√©gale'], \n",
    "    prefix=['APE', 'CJ'],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# 4. S√âLECTION FINALE ET NETTOYAGE DES COLONNES INUTILES\n",
    "cols_to_drop = [\n",
    "    'SIREN', 'Code postal de l\\'√©tablissement', 'Code commune de l\\'√©tablissement',\n",
    "    'D√©nomination de l\\'unit√© l√©gale', 'Activit√© principale de l\\'unit√© l√©gale',\n",
    "    'Date_fermeture_finale', 'latitude', 'longitude', 'code_ape',\n",
    "    'Code du d√©partement de l\\'√©tablissement', 'Code de la r√©gion de l\\'√©tablissement',\n",
    "    'Economie sociale et solidaire unit√© l√©gale'\n",
    "]\n",
    "\n",
    "df_final = df_final.drop(columns=[c for c in cols_to_drop if c in df_final.columns])\n",
    "\n",
    "# 5. DERNIERS R√âGLAGES POUR LA SURVIE\n",
    "\n",
    "df_final['fermeture'] = df_final['fermeture'].astype(bool)\n",
    "df_final = df_final[df_final['age_estime'] > 0]\n",
    "\n",
    "print(f\"‚úÖ Dataset finalis√© : {df_final.shape[0]} lignes, {df_final.shape[1]} colonnes.\")\n",
    "display(df_final.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f73d2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcul de la fr√©quence pour les colonnes binaires (APE et CJ)\n",
    "binary_cols = [c for c in df_final.columns if c.startswith('APE_') or c.startswith('CJ_')]\n",
    "frequencies = df_final[binary_cols].mean().sort_values(ascending=False) * 100\n",
    "\n",
    "# 2. Visualisation des 30 cat√©gories les plus rares vs les plus fr√©quentes\n",
    "\n",
    "# 3. Focus sur les \"Micro-Cat√©gories\"\n",
    "rare_limit = 0.1 # Seuil de 0.1%\n",
    "rare_cols = frequencies[frequencies < rare_limit]\n",
    "\n",
    "print(f\"--- üîç Analyse des colonnes rares (< {rare_limit}%) ---\")\n",
    "print(f\"Il y a {len(rare_cols)} colonnes qui concernent moins de 0.1% du dataset.\")\n",
    "if len(rare_cols) > 0:\n",
    "    print(rare_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556ee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identifier les colonnes √† fusionner par famille\n",
    "rare_ape_cols = [c for c in rare_cols.index if c.startswith('APE_')]\n",
    "rare_cj_cols = [c for c in rare_cols.index if c.startswith('CJ_')]\n",
    "\n",
    "# 2. Cr√©er la colonne \"Autres\" pour les APE\n",
    "if rare_ape_cols:\n",
    "    df_final['APE_Autres_Secteurs'] = df_final[rare_ape_cols].any(axis=1)\n",
    "    df_final.drop(columns=rare_ape_cols, inplace=True)\n",
    "\n",
    "# 3. Cr√©er la colonne \"Autres\" pour les CJ\n",
    "if rare_cj_cols:\n",
    "    df_final['CJ_Autres_Status'] = df_final[rare_cj_cols].any(axis=1)\n",
    "    df_final.drop(columns=rare_cj_cols, inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Nettoyage termin√©. Nouveau nombre de colonnes : {len(df_final.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34904c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcul des fr√©quences en nombre d'entreprises\n",
    "binary_cols = [c for c in df_final.columns if c.startswith('APE_')]\n",
    "counts = df_final[binary_cols].sum().sort_values(ascending=True)\n",
    "\n",
    "niche_sectors = counts[counts < 6000] \n",
    "\n",
    "fig_niche = px.bar(\n",
    "    x=niche_sectors.values, \n",
    "    y=niche_sectors.index,\n",
    "    orientation='h',\n",
    "    title=f\"üîç Zoom sur les {len(niche_sectors)} secteurs de niche (< 0.5% du total)\",\n",
    "    labels={'x': 'Nombre total d\\'√©tablissements', 'y': 'Secteur APE'},\n",
    "    color=niche_sectors.values,\n",
    "    color_continuous_scale='Reds_r'\n",
    ")\n",
    "\n",
    "fig_niche.update_layout(\n",
    "    height=800, \n",
    "    margin=dict(l=400),\n",
    "    xaxis_title=\"Nombre d'entreprises (Volume r√©el)\"\n",
    ")\n",
    "\n",
    "fig_niche.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a60d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b482d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059db7ac",
   "metadata": {},
   "source": [
    "##### Visu des facteurs d'influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_target = df_final.corr()['fermeture'].sort_values(ascending=False).to_frame()\n",
    "\n",
    "fig_corr = go.Figure(data=go.Heatmap(\n",
    "    z=corr_target.values,\n",
    "    x=['Corr√©lation'],\n",
    "    y=corr_target.index,\n",
    "    colorscale='RdBu_r',\n",
    "    zmin=-0.15, zmax=0.15,\n",
    "    text=corr_target.values.round(3),\n",
    "    texttemplate=\"%{text}\",\n",
    "    showscale=True\n",
    "))\n",
    "\n",
    "fig_corr.update_layout(\n",
    "    title=\"üéØ Focus : Impact des variables sur la Fermeture\",\n",
    "    height=1400,\n",
    "    width=800,\n",
    "    margin=dict(l=450, r=50, t=100, b=100),\n",
    "    yaxis=dict(\n",
    "        side='left',\n",
    "        tickfont=dict(size=12),\n",
    "        autorange=\"reversed\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_corr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9b2b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306354d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pr√©paration des matrices (X = features, y = structur√© pour la survie)\n",
    "X = df_final.drop(columns=['fermeture', 'age_estime'])\n",
    "\n",
    "# sksurv a besoin d'un format sp√©cifique : un tableau de tuples (Event, Time)\n",
    "y = np.array([(bool(f), float(a)) for f, a in zip(df_final['fermeture'], df_final['age_estime'])],\n",
    "             dtype=[('event', 'bool'), ('time', 'float')])\n",
    "\n",
    "# 2. Split 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Lancement du Run MLflow\n",
    "mlflow.set_experiment(\"Survie_Entreprises_V1\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Baseline_RSF_57_cols\"):\n",
    "    # On commence \"l√©ger\" pour ne pas faire chauffer la machine\n",
    "    rf_params = {\n",
    "        \"n_estimators\": 50,\n",
    "        \"max_depth\": 8,\n",
    "        \"min_samples_split\": 100,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "    mlflow.log_params(rf_params)\n",
    "    \n",
    "    rsf = RandomSurvivalForest(**rf_params)\n",
    "    rsf.fit(X_train, y_train)\n",
    "    \n",
    "    # Score de concordance (C-index)\n",
    "    c_index = rsf.score(X_test, y_test)\n",
    "    mlflow.log_metric(\"c_index\", c_index)\n",
    "    \n",
    "    # Sauvegarde\n",
    "    mlflow.sklearn.log_model(rsf, \"model_rsf\")\n",
    "    \n",
    "    print(f\"‚úÖ Entra√Ænement termin√© ! C-Index: {c_index:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a339f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. D√©finir les points temporels (en mois)\n",
    "times = [12, 24, 36]\n",
    "\n",
    "# 2. Pr√©dire les fonctions de survie sur le jeu de test\n",
    "\n",
    "surv_funcs = rsf.predict_survival_function(X_test)\n",
    "\n",
    "# 3. Extraire les probabilit√©s de FERMETURE (1 - Survie)\n",
    "\n",
    "risques_data = []\n",
    "for fn in surv_funcs:\n",
    "    risques_data.append({\n",
    "        \"Risque_1_an\": round((1 - fn(12)) * 100, 2),\n",
    "        \"Risque_2_ans\": round((1 - fn(24)) * 100, 2),\n",
    "        \"Risque_3_ans\": round((1 - fn(36)) * 100, 2)\n",
    "    })\n",
    "\n",
    "df_predictions = pd.DataFrame(risques_data, index=X_test.index)\n",
    "\n",
    "# 4. Afficher un aper√ßu\n",
    "print(\"üìã Aper√ßu des probabilit√©s de fermeture (%) :\")\n",
    "display(df_predictions.head(10))\n",
    "\n",
    "# 5. Petite stat rapide pour voir si c'est coh√©rent\n",
    "print(\"\\nüìä Risque moyen par horizon :\")\n",
    "print(df_predictions.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6973823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. On fusionne les pr√©dictions avec les caract√©ristiques d'origine\n",
    "df_analyse = df_predictions.merge(X_test, left_index=True, right_index=True)\n",
    "\n",
    "# 2. On compare les moyennes des deux groupes\n",
    "high_risk = df_analyse[df_analyse['Risque_3_ans'] > 90].mean()\n",
    "low_risk = df_analyse[df_analyse['Risque_3_ans'] < 75].mean()\n",
    "\n",
    "comparison = pd.DataFrame({'Ultra-Risque (>90%)': high_risk, 'Plus Solides (<75%)': low_risk})\n",
    "print(\"üîç Comparaison des profils types :\")\n",
    "display(comparison.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. On r√©cup√®re les lignes de test avec le risque_departemental d'origine\n",
    "df_geo_check = df_analyse[['Risque_3_ans', 'risque_departemental']].copy()\n",
    "\n",
    "# 2. On cr√©e des groupes de risque pour y voir plus clair\n",
    "df_geo_check['Tranche_Risque'] = pd.qcut(df_geo_check['Risque_3_ans'], q=5, \n",
    "                                         labels=['Tr√®s Faible', 'Faible', 'Moyen', 'Fort', 'Critique'])\n",
    "\n",
    "# 3. On regarde la valeur moyenne du score d√©partemental pour chaque tranche\n",
    "geo_stats = df_geo_check.groupby('Tranche_Risque')['risque_departemental'].mean()\n",
    "\n",
    "print(\"üìç Score de risque d√©partemental moyen par tranche de pr√©diction :\")\n",
    "print(geo_stats)\n",
    "\n",
    "# 4. Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=geo_stats.index, y=geo_stats.values, palette='OrRd')\n",
    "plt.title('Validation : Le poids de la g√©ographie dans la pr√©diction finale')\n",
    "plt.ylabel('Moyenne du Score Risque D√©partemental (Target Encoded)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On trie pour avoir les extr√™mes\n",
    "top_solides = df_analyse.sort_values('Risque_3_ans', ascending=True).head(10)\n",
    "top_fragiles = df_analyse.sort_values('Risque_3_ans', ascending=False).head(10)\n",
    "\n",
    "print(\"üíé TOP 10 : Les Entreprises les plus robustes\")\n",
    "display(top_solides[['Risque_1_an', 'Risque_3_ans', 'Tranche_effectif_num', 'risque_departemental']])\n",
    "\n",
    "print(\"\\nüî• TOP 10 : Les Entreprises les plus fragiles\")\n",
    "display(top_fragiles[['Risque_1_an', 'Risque_3_ans', 'Tranche_effectif_num', 'risque_departemental']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d05b5",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-comptable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
