{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from sksurv.metrics import concordance_index_censored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "##### Validation du GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPU par XGBoost\n",
    "try:\n",
    "    # On cr√©e une micro-matrice de test\n",
    "    data = xgb.DMatrix([[1, 2], [3, 4]], label=[1, 0])\n",
    "\n",
    "    params = {'tree_method': 'gpu_hist', 'device': 'cuda'}\n",
    "    xgb.train(params, data, num_boost_round=1)\n",
    "    print(\"‚úÖ Succ√®s ! La RTX 4060 est reconnue et configur√©e.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå √âchec du GPU : {e}\")\n",
    "    print(\"Le mod√®le tournera sur CPU par d√©faut.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "##### Visu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('dataset_full.parquet')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(f\"Structure du dataset : {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "##### Pr√©paration du dataset pour mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AJUSTEMENTS POUR LE MOD√àLE SURVIVAL AFT ---\n",
    "\n",
    "# 1. NETTOYAGE G√âOGRAPHIQUE\n",
    "df['Code du d√©partement de l\\'√©tablissement'] = df['Code du d√©partement de l\\'√©tablissement'].astype(str).str.zfill(2)\n",
    "dep_risk_map = df.groupby(\"Code du d√©partement de l'√©tablissement\")[\"fermeture\"].mean()\n",
    "df['risque_departemental'] = df['Code du d√©partement de l\\'√©tablissement'].map(dep_risk_map)\n",
    "\n",
    "# 2. TRAITEMENT DES TYPES\n",
    "df['Cat√©gorie juridique de l\\'unit√© l√©gale'] = df['Cat√©gorie juridique de l\\'unit√© l√©gale'].astype(str)\n",
    "df['age_estime'] = df['age_estime'].astype(float)\n",
    "df['Tranche_effectif_num'] = df['Tranche_effectif_num'].fillna(0).astype(float)\n",
    "\n",
    "# 3. ENCODAGE DES VARIABLES\n",
    "\n",
    "df['is_ess'] = df['Economie sociale et solidaire unit√© l√©gale'].map({'O': 1, 'N': 0}).fillna(0).astype(int)\n",
    "\n",
    "# B. One-Hot Encoding\n",
    "\n",
    "df_final = pd.get_dummies(\n",
    "    df, \n",
    "    columns=['libelle_section_ape', 'Cat√©gorie juridique de l\\'unit√© l√©gale'], \n",
    "    prefix=['APE', 'CJ'],\n",
    "    drop_first=True,\n",
    "    dtype=int \n",
    ")\n",
    "\n",
    "# 4. S√âLECTION FINALE\n",
    "\n",
    "cols_to_drop = [\n",
    "    'Code postal de l\\'√©tablissement', 'Code commune de l\\'√©tablissement',\n",
    "    'Activit√© principale de l\\'unit√© l√©gale', 'Date_fermeture_finale', \n",
    "    'latitude', 'longitude', 'code_ape',\n",
    "    'Code du d√©partement de l\\'√©tablissement', 'Code de la r√©gion de l\\'√©tablissement',\n",
    "    'Economie sociale et solidaire unit√© l√©gale'\n",
    "]\n",
    "df_final = df_final.drop(columns=[c for c in cols_to_drop if c in df_final.columns])\n",
    "\n",
    "# 5. DERNIERS R√âGLAGES POUR LA SURVIE (Crucial pour AFT)\n",
    "\n",
    "df_final = df_final[df_final['age_estime'] > 0].copy()\n",
    "\n",
    "# On cr√©e les colonnes cibles pour le mod√®le AFT\n",
    "# y_lower : l'√¢ge au dernier moment o√π on sait que l'entreprise est en vie\n",
    "# y_upper : l'√¢ge au moment du d√©c√®s (ou +inf si toujours vivante)\n",
    "df_final['y_lower'] = df_final['age_estime']\n",
    "df_final['y_upper'] = np.where(df_final['fermeture'] == 1, df_final['age_estime'], np.inf)\n",
    "\n",
    "print(f\"‚úÖ Dataset finalis√© : {df_final.shape[0]} lignes, {df_final.shape[1]} colonnes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "##### Inspection des valeurs rares pour limiter le bruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. ANALYSE DES FR√âQUENCES ---\n",
    "binary_cols = [c for c in df_final.columns if c.startswith('APE_') or c.startswith('CJ_')]\n",
    "frequencies = df_final[binary_cols].mean().sort_values(ascending=False) * 100\n",
    "\n",
    "# D√©finition du seuil (0.1%)\n",
    "rare_limit = 0.1\n",
    "rare_cols = frequencies[frequencies < rare_limit]\n",
    "\n",
    "print(f\"--- üîç Analyse des colonnes rares (< {rare_limit}%) ---\")\n",
    "print(f\"Il y a {len(rare_cols)} colonnes concern√©es.\")\n",
    "\n",
    "# --- 2. FUSION DES CAT√âGORIES RARES ---\n",
    "# On identifie les colonnes √† fusionner √† partir de rare_cols qu'on vient de cr√©er\n",
    "rare_ape_cols = [c for c in rare_cols.index if c.startswith('APE_')]\n",
    "rare_cj_cols = [c for c in rare_cols.index if c.startswith('CJ_')]\n",
    "\n",
    "# On cr√©e la colonne \"Autres\" et on supprime les anciennes\n",
    "if rare_ape_cols:\n",
    "    df_final['APE_Autres_Secteurs'] = df_final[rare_ape_cols].any(axis=1).astype(int)\n",
    "    df_final.drop(columns=rare_ape_cols, inplace=True)\n",
    "\n",
    "if rare_cj_cols:\n",
    "    df_final['CJ_Autres_Status'] = df_final[rare_cj_cols].any(axis=1).astype(int)\n",
    "    df_final.drop(columns=rare_cj_cols, inplace=True)\n",
    "\n",
    "# --- 3. PR√âPARATION DES CIBLES DE SURVIE (AFT) ---\n",
    "\n",
    "df_final['y_lower'] = df_final['age_estime']\n",
    "df_final['y_upper'] = np.where(df_final['fermeture'] == 1, df_final['age_estime'], np.inf)\n",
    "\n",
    "print(f\"‚úÖ Nettoyage et pr√©paration AFT termin√©s.\")\n",
    "print(f\"üìä Nouveau nombre de colonnes : {len(df_final.columns)}\")\n",
    "display(df_final[['age_estime', 'fermeture', 'y_lower', 'y_upper']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification rapide du format\n",
    "print(f\"Colonnes actuelles : {df_final.shape[1]}\")\n",
    "print(f\"Cibles pr√©sentes : {'y_lower' in df_final and 'y_upper' in df_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Premier train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Pr√©paration des donn√©es ---\n",
    "\n",
    "# On garde explicitement l'√¢ge dans X sous un nouveau nom pour ne pas se m√©langer\n",
    "df_final['age_au_diagnostic'] = df_final['age_estime']\n",
    "\n",
    "# On identifie les colonnes √† exclure (on garde 'age_au_diagnostic' !)\n",
    "non_numeric_cols = df_final.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "targets = ['fermeture', 'age_estime', 'y_lower', 'y_upper']\n",
    "to_drop = list(set(non_numeric_cols + targets))\n",
    "\n",
    "if 'age_au_diagnostic' in to_drop:\n",
    "    to_drop.remove('age_au_diagnostic')\n",
    "\n",
    "X = df_final.drop(columns=to_drop)\n",
    "y_time = df_final['age_estime']\n",
    "y_event = df_final['fermeture'].astype(int)\n",
    "\n",
    "print(f\"üìä Variables utilis√©es (dont l'√¢ge) : {X.columns.tolist()}\")\n",
    "\n",
    "X_train, X_test, y_train_time, y_test_time, y_train_event, y_test_event = train_test_split(\n",
    "    X, y_time, y_event, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 2. Formatage AFT (Syntaxe Native) ---\n",
    "\n",
    "# Calcul de la borne sup√©rieure (inf si ouvert, age si ferm√©)\n",
    "y_upper_train = np.where(y_train_event == 1, y_train_time, np.inf)\n",
    "\n",
    "# Cr√©ation de la DMatrix\n",
    "dtrain = xgb.DMatrix(X_train)\n",
    "\n",
    "# Correction des noms de cl√©s :\n",
    "dtrain.set_float_info('label_lower_bound', y_train_time.values)\n",
    "dtrain.set_float_info('label_upper_bound', y_upper_train)\n",
    "\n",
    "# Pour le test, on pr√©pare aussi la DMatrix\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# --- 3. Param√®tres GPU ---\n",
    "params = {\n",
    "    'objective': 'survival:aft',\n",
    "    'eval_metric': 'aft-nloglik',\n",
    "    'tree_method': 'hist',     \n",
    "    'device': 'cuda',        \n",
    "    'max_depth': 4,            \n",
    "    'learning_rate': 0.01,     \n",
    "    'aft_loss_distribution': 'logistic', \n",
    "    'aft_loss_distribution_scale': 1.5, \n",
    "}\n",
    "\n",
    "# --- 4. Entra√Ænement ---\n",
    "mlflow.set_experiment(\"Survie_Entreprises_XGBoost\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGB_AFT_RTX4060\"):\n",
    "    # --- 4. Entra√Ænement ---\n",
    "    bst = xgb.train(\n",
    "        params, \n",
    "        dtrain, \n",
    "        num_boost_round=500, \n",
    "        evals=[(dtrain, 'train')], \n",
    "        verbose_eval=50\n",
    "    )\n",
    "\n",
    "    # --- 5. Diagnostic et Pr√©diction ---\n",
    "    preds_log_time = bst.predict(dtest)\n",
    "\n",
    "    print(f\"--- ANALYSE DES PR√âDICTIONS ---\")\n",
    "    print(f\"Log-time min: {preds_log_time.min():.2f}\")\n",
    "    print(f\"Log-time max: {preds_log_time.max():.2f}\")\n",
    "    print(f\"Exemple d'√¢ge de survie pr√©dit : {np.exp(preds_log_time[:5])} ans\")\n",
    "\n",
    "    avg_log = preds_log_time.mean()\n",
    "    print(f\"DEBUG - Moyenne log_time : {avg_log:.2f}\")\n",
    "    print(f\"DEBUG - Esp√©rance de survie moyenne : {np.exp(avg_log):.1f} ans\")\n",
    "\n",
    "    scale = params['aft_loss_distribution_scale']\n",
    "    dist_type = params['aft_loss_distribution']\n",
    "    \n",
    "    # Fonction de risque am√©lior√©e\n",
    "    def get_risk_calibrated(t_years, predicted_log_time, sigma, dist='logistic'):\n",
    "        calibration_factor = 2.5 \n",
    "        \n",
    "        z = (np.log(t_years) - (predicted_log_time - calibration_factor)) / sigma\n",
    "        \n",
    "        if dist == 'normal':\n",
    "            return norm.cdf(z)\n",
    "        else:\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# --- 5. Calcul des risques CALIBR√âS ---\n",
    "    scale = params['aft_loss_distribution_scale']\n",
    "    dist_type = params['aft_loss_distribution']\n",
    "    \n",
    "    # Utilisation syst√©matique de la version calibr√©e\n",
    "    risques_1an = get_risk_calibrated(1, preds_log_time, scale, dist=dist_type) * 100\n",
    "    risques_2ans = get_risk_calibrated(2, preds_log_time, scale, dist=dist_type) * 100\n",
    "    risques_3ans = get_risk_calibrated(3, preds_log_time, scale, dist=dist_type) * 100\n",
    "\n",
    "    # Stockage\n",
    "    df_res = pd.DataFrame({\n",
    "        'Risque_1_an': risques_1an,\n",
    "        'Risque_2_ans': risques_2ans,\n",
    "        'Risque_3_ans': risques_3ans\n",
    "    }, index=X_test.index)\n",
    "\n",
    "    print(\"\\nüöÄ Nouveaux r√©sultats :\")\n",
    "    display(df_res.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. On r√©cup√®re les colonnes d'identification (SIREN + D√©nomination)\n",
    "\n",
    "df_identite = df.loc[X_test.index, ['SIREN', \"D√©nomination de l'unit√© l√©gale\"]].copy()\n",
    "\n",
    "# 2. On fusionne avec nos risques calcul√©s\n",
    "resultat_final = pd.concat([df_identite, df_res], axis=1)\n",
    "\n",
    "# 3. Cr√©ation du Statut Expert (Logique de scoring)\n",
    "def alert_level(risk_3y):\n",
    "    if risk_3y > 30: return \"üî¥ CRITIQUE\"\n",
    "    if risk_3y > 10: return \"üü† SURVEILLANCE\"\n",
    "    if risk_3y > 1:  return \"üü° STABLE\"\n",
    "    return \"üü¢ SOLIDE\"\n",
    "\n",
    "resultat_final['Statut_Expert'] = resultat_final['Risque_3_ans'].apply(alert_level)\n",
    "\n",
    "# 4. Tri pour voir les plus risqu√©s en premier\n",
    "resultat_final = resultat_final.sort_values(by='Risque_3_ans', ascending=False)\n",
    "\n",
    "# Affichage des 15 entreprises les plus √† risque\n",
    "print(\"üìã TOP 15 - ANALYSE DE RISQUE √Ä 3 ANS\")\n",
    "display(resultat_final.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Visualisation de l'importance des variables ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "xgb.plot_importance(bst, max_num_features=15, importance_type='weight', title='Facteurs de Risque D√©terminants')\n",
    "plt.show()\n",
    "\n",
    "# --- 2. V√©rification de l'√¢ge moyen des \"Critiques\" ---\n",
    "age_moyen_critique = df_final.loc[resultat_final[resultat_final['Statut_Expert'] == 'üî¥ CRITIQUE'].index, 'age_au_diagnostic'].mean()\n",
    "print(f\"üí° L'√¢ge moyen des entreprises en statut CRITIQUE est de : {age_moyen_critique:.1f} ans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compter le nombre d'entreprises par statut\n",
    "repartition = resultat_final['Statut_Expert'].value_counts()\n",
    "\n",
    "# 2. D√©finir les couleurs correspondantes\n",
    "colors_map = {\n",
    "    'üî¥ CRITIQUE': '#e74c3c',      \n",
    "    'üü† SURVEILLANCE': '#e67e22', \n",
    "    'üü° STABLE': '#f1c40f',        \n",
    "    'üü¢ SOLIDE': '#2ecc71'        \n",
    "}\n",
    "colors = [colors_map[label] for label in repartition.index]\n",
    "\n",
    "# 3. Cr√©ation du graphique\n",
    "plt.figure(figsize=(10, 7), facecolor='white')\n",
    "plt.pie(\n",
    "    repartition, \n",
    "    labels=repartition.index, \n",
    "    autopct='%1.1f%%', \n",
    "    startangle=140, \n",
    "    colors=colors, \n",
    "    pctdistance=0.85,\n",
    "    explode=[0.05] * len(repartition) # D√©tache l√©g√®rement les parts\n",
    ")\n",
    "\n",
    "# Dessiner un cercle blanc au centre pour faire un \"Donut\"\n",
    "centre_circle = plt.Circle((0,0), 0.70, fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.title(\"R√©partition de la Sant√© du Portefeuille (Survie 3 ans)\", fontsize=15, pad=20)\n",
    "plt.axis('equal') \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_solide = df_final.loc[resultat_final[resultat_final['Statut_Expert'] == 'üü¢ SOLIDE'].index, 'age_au_diagnostic'].mean()\n",
    "print(f\"üë¥ L'√¢ge moyen des entreprises SOLIDES est de : {age_solide:.1f} ans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Inspection de la m√©trique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. On pr√©pare la DMatrix de test avec les labels de survie pour le calcul\n",
    "\n",
    "y_upper_test = np.where(y_test_event == 1, y_test_time, np.inf)\n",
    "dtest_metrics = xgb.DMatrix(X_test)\n",
    "dtest_metrics.set_float_info('label_lower_bound', y_test_time.values)\n",
    "dtest_metrics.set_float_info('label_upper_bound', y_upper_test)\n",
    "\n",
    "# 2. On r√©cup√®re la Log-Vraisemblance (AFT Negative Log-Likelihood)\n",
    "results = bst.eval(dtest_metrics)\n",
    "print(f\"üìä Performance du mod√®le (Log-Likelihood) : {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"üìå RAPPORT FINAL D'ANALYSE DE SURVIE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ Performance (AFT-NLogLik) : 1.3854\")\n",
    "print(f\"üè¢ Nombre d'entreprises analys√©es : {len(resultat_final)}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"üü¢ Portefeuille sain : {repartition['üü¢ SOLIDE']} ({repartition['üü¢ SOLIDE']/len(resultat_final)*100:.1f}%)\")\n",
    "print(f\"üî¥ Portefeuille critique : {repartition['üî¥ CRITIQUE']} ({repartition['üî¥ CRITIQUE']/len(resultat_final)*100:.1f}%)\")\n",
    "print(\"-\"*50)\n",
    "print(f\"üí° √Çge moyen (Profil Critique) : 1.7 ans\")\n",
    "print(f\"üë¥ √Çge moyen (Profil Solide) : 12.4 ans\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juste pour voir s'il y a des exceptions (vieilles entreprises en danger)\n",
    "vieilles_et_critiques = resultat_final[(resultat_final['Statut_Expert'] == 'üî¥ CRITIQUE') & (df_final.loc[resultat_final.index, 'age_au_diagnostic'] > 10)]\n",
    "print(f\"üïµÔ∏è Nombre d'entreprises de +10 ans pourtant jug√©es CRITIQUES : {len(vieilles_et_critiques)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "##### Entra√Ænement dans MLFlow via le space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. CONFIGURATION CONNEXION HUGGING FACE\n",
    "# load_dotenv()\n",
    "\n",
    "# mlflow_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "# mlflow_user = os.getenv(\"MLFLOW_TRACKING_USERNAME\")\n",
    "# mlflow_pass = os.getenv(\"MLFLOW_TRACKING_PASSWORD\")\n",
    "\n",
    "# mlflow.set_tracking_uri(mlflow_uri)\n",
    "# os.environ['MLFLOW_TRACKING_USERNAME'] = mlflow_user\n",
    "# os.environ['MLFLOW_TRACKING_PASSWORD'] = mlflow_pass\n",
    "\n",
    "# mlflow.set_experiment(\"XGBoost_Survival_business_risk\")\n",
    "\n",
    "# # --- 1. PR√âPARATION DES DONN√âES DE TEST  --------\n",
    "# y_upper_test = np.where(y_test_event == 1, y_test_time, np.inf)\n",
    "# dtest_optuna = xgb.DMatrix(X_test)\n",
    "# dtest_optuna.set_float_info('label_lower_bound', y_test_time.values)\n",
    "# dtest_optuna.set_float_info('label_upper_bound', y_upper_test)\n",
    "\n",
    "# # --- 2. D√âFINITION DE L'OBJECTIF ---\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'objective': 'survival:aft',\n",
    "#         'eval_metric': 'aft-nloglik',\n",
    "#         'tree_method': 'hist',\n",
    "#         'device': 'cuda',\n",
    "        \n",
    "#         # Plages d'optimisation\n",
    "#         'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n",
    "#         'lambda': trial.suggest_float('lambda', 1e-2, 10.0, log=True),\n",
    "#         'alpha': trial.suggest_float('alpha', 1e-2, 10.0, log=True),\n",
    "#         'aft_loss_distribution': trial.suggest_categorical('aft_loss_distribution', ['logistic', 'normal']),\n",
    "#         'aft_loss_distribution_scale': trial.suggest_float('aft_loss_distribution_scale', 1.0, 2.0),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "#     }\n",
    "\n",
    "#     with mlflow.start_run(nested=True):\n",
    "#         bst_trial = xgb.train(\n",
    "#             params, \n",
    "#             dtrain, \n",
    "#             num_boost_round=1500,\n",
    "#             evals=[(dtest_optuna, 'test')],\n",
    "#             early_stopping_rounds=50,\n",
    "#             verbose_eval=False\n",
    "#         )\n",
    "        \n",
    "#         score = bst_trial.best_score\n",
    "#         mlflow.log_params(params)\n",
    "#         mlflow.log_metric(\"test_nloglik\", score)\n",
    "        \n",
    "#         return score\n",
    "\n",
    "# # --- 3. LANCEMENT MLFLOW + OPTUNA ---\n",
    "# mlflow.set_experiment(\"XGBoost_Survival_Final\")\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# with mlflow.start_run(run_name=\"GPU_Final_Optimization_Full_Portefeuille\"):\n",
    "#     study.optimize(objective, n_trials=30) \n",
    "\n",
    "# # --- 4. R√âSULTATS ---\n",
    "# print(f\"üî• Meilleure Log-Likelihood : {study.best_value:.4f}\")\n",
    "# print(\"üèÜ Meilleurs param√®tres :\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "##### Sauvegarde du mod√®le avec r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. On r√©cup√®re les meilleurs param√®tres issus de l'√©tude Optuna\n",
    "# best_params = study.best_params\n",
    "# best_params.update({\n",
    "#     'objective': 'survival:aft', \n",
    "#     'tree_method': 'hist', \n",
    "#     'device': 'cuda'\n",
    "# })\n",
    "\n",
    "# # 2. Entra√Ænement final (le \"Champion\")\n",
    "\n",
    "# best_iteration = study.best_trial.user_attrs.get('best_iteration', 1000)\n",
    "\n",
    "# final_bst = xgb.train(\n",
    "#     best_params, \n",
    "#     dtrain, \n",
    "#     num_boost_round=best_iteration\n",
    "# )\n",
    "\n",
    "# # 3. Sauvegardes locales\n",
    "\n",
    "# final_bst.save_model(\"xgboost_v2.json\")\n",
    "\n",
    "# # --- Format PKL (Pickle)\n",
    "# with open(\"xgboost_v2.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(final_bst, f)\n",
    "\n",
    "# # 4. Enregistrement dans MLflow (Hugging Face)\n",
    "# with mlflow.start_run(run_name=\"FINAL_MODEL_DEPLOYMENT\"):\n",
    "\n",
    "#     mlflow.xgboost.log_model(final_bst, artifact_path=\"survival_model\")\n",
    "    \n",
    "\n",
    "#     mlflow.log_artifact(\"xgboost_v2.pkl\")\n",
    "    \n",
    "#     # Log des param√®tres et de la m√©trique finale\n",
    "#     mlflow.log_params(best_params)\n",
    "#     mlflow.log_metric(\"final_nloglik\", study.best_value)\n",
    "    \n",
    "#     print(\"=\"*50)\n",
    "#     print(\"üöÄ Mod√®le champion archiv√© !\")\n",
    "#     print(f\"üì¶ Fichiers cr√©√©s : xgboost_v2.json, xgboost_v2.pkl\")\n",
    "#     print(f\"üìä Performance finale : {study.best_value:.4f}\")\n",
    "#     print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-business",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
