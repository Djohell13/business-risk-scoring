{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from sksurv.metrics import concordance_index_censored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "#### Test GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPU par XGBoost\n",
    "try:\n",
    "    # On cr√©e une micro-matrice de test\n",
    "    data = xgb.DMatrix([[1, 2], [3, 4]], label=[1, 0])\n",
    "\n",
    "    params = {'tree_method': 'gpu_hist', 'device': 'cuda'}\n",
    "    xgb.train(params, data, num_boost_round=1)\n",
    "    print(\"‚úÖ Succ√®s ! La RTX 4060 est reconnue et configur√©e.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå √âchec du GPU : {e}\")\n",
    "    print(\"Le mod√®le tournera sur CPU par d√©faut.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('dataset_full.parquet')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(f\"Structure du dataset : {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "##### Pr√©paration dataset pour mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. NETTOYAGE G√âOGRAPHIQUE & RISQUE D√âPARTEMENTAL\n",
    "\n",
    "df['Code du d√©partement de l\\'√©tablissement'] = df['Code du d√©partement de l\\'√©tablissement'].astype(str).str.zfill(2)\n",
    "\n",
    "dep_risk_map = df.groupby(\"Code du d√©partement de l'√©tablissement\")[\"fermeture\"].mean()\n",
    "df['risque_departemental'] = df['Code du d√©partement de l\\'√©tablissement'].map(dep_risk_map)\n",
    "\n",
    "# 2. TRAITEMENT DES TYPES (Cat√©gories et Nullables)\n",
    "\n",
    "df['Cat√©gorie juridique de l\\'unit√© l√©gale'] = df['Cat√©gorie juridique de l\\'unit√© l√©gale'].astype(str)\n",
    "\n",
    "\n",
    "df['age_estime'] = df['age_estime'].astype(float)\n",
    "df['Tranche_effectif_num'] = df['Tranche_effectif_num'].fillna(0).astype(float)\n",
    "\n",
    "# 3. ENCODAGE DES VARIABLES\n",
    "\n",
    "df['is_ess'] = df['Economie sociale et solidaire unit√© l√©gale'].map({'O': 1, 'N': 0}).fillna(0)\n",
    "\n",
    "# B. One-Hot Encoding : Secteurs APE et Cat√©gories Juridiques\n",
    "\n",
    "df_final = pd.get_dummies(\n",
    "    df, \n",
    "    columns=['libelle_section_ape', 'Cat√©gorie juridique de l\\'unit√© l√©gale'], \n",
    "    prefix=['APE', 'CJ'],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# 4. S√âLECTION FINALE ET NETTOYAGE DES COLONNES INUTILES\n",
    "cols_to_drop = [\n",
    "    'SIREN', 'Code postal de l\\'√©tablissement', 'Code commune de l\\'√©tablissement',\n",
    "    'D√©nomination de l\\'unit√© l√©gale', 'Activit√© principale de l\\'unit√© l√©gale',\n",
    "    'Date_fermeture_finale', 'latitude', 'longitude', 'code_ape',\n",
    "    'Code du d√©partement de l\\'√©tablissement', 'Code de la r√©gion de l\\'√©tablissement',\n",
    "    'Economie sociale et solidaire unit√© l√©gale'\n",
    "]\n",
    "\n",
    "df_final = df_final.drop(columns=[c for c in cols_to_drop if c in df_final.columns])\n",
    "\n",
    "# 5. DERNIERS R√âGLAGES POUR LA SURVIE\n",
    "\n",
    "df_final['fermeture'] = df_final['fermeture'].astype(bool)\n",
    "\n",
    "df_final = df_final[df_final['age_estime'] > 0]\n",
    "\n",
    "print(f\"‚úÖ Dataset finalis√© : {df_final.shape[0]} lignes, {df_final.shape[1]} colonnes.\")\n",
    "display(df_final.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "##### Analyse des colonnes rares pour limiter le bruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcul de la fr√©quence pour les colonnes binaires (APE et CJ)\n",
    "binary_cols = [c for c in df_final.columns if c.startswith('APE_') or c.startswith('CJ_')]\n",
    "frequencies = df_final[binary_cols].mean().sort_values(ascending=False) * 100\n",
    "\n",
    "# 2. Visualisation des 30 cat√©gories les plus rares vs les plus fr√©quentes\n",
    "\n",
    "# 3. Focus sur les \"Micro-Cat√©gories\"\n",
    "rare_limit = 0.1\n",
    "rare_cols = frequencies[frequencies < rare_limit]\n",
    "\n",
    "print(f\"--- üîç Analyse des colonnes rares (< {rare_limit}%) ---\")\n",
    "print(f\"Il y a {len(rare_cols)} colonnes qui concernent moins de 0.1% du dataset.\")\n",
    "if len(rare_cols) > 0:\n",
    "    print(rare_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "##### Nettoyage des colonnes rares par regroupage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identifier les colonnes √† fusionner par famille\n",
    "rare_ape_cols = [c for c in rare_cols.index if c.startswith('APE_')]\n",
    "rare_cj_cols = [c for c in rare_cols.index if c.startswith('CJ_')]\n",
    "\n",
    "# 2. Cr√©er la colonne \"Autres\" pour les APE\n",
    "if rare_ape_cols:\n",
    "\n",
    "    df_final['APE_Autres_Secteurs'] = df_final[rare_ape_cols].any(axis=1)\n",
    "    df_final.drop(columns=rare_ape_cols, inplace=True)\n",
    "\n",
    "# 3. Cr√©er la colonne \"Autres\" pour les CJ\n",
    "if rare_cj_cols:\n",
    "    df_final['CJ_Autres_Status'] = df_final[rare_cj_cols].any(axis=1)\n",
    "    df_final.drop(columns=rare_cj_cols, inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Nettoyage termin√©. Nouveau nombre de colonnes : {len(df_final.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "##### Train-test-val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition des cibles\n",
    "X = df_final.drop(columns=['fermeture', 'age_estime'])\n",
    "y_time = df_final['age_estime']\n",
    "y_event = df_final['fermeture']\n",
    "\n",
    "# √âtape A : On isole 15% pour le Test final\n",
    "X_temp, X_test, y_time_temp, y_test_time, y_event_temp, y_test_event = train_test_split(\n",
    "    X, y_time, y_event, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# √âtape B : Dans les 85% restants, on prend 17.6% pour la Validation \n",
    "# (ce qui repr√©sente 15% du total initial)\n",
    "X_train, X_val, y_train_time, y_val_time, y_train_event, y_val_event = train_test_split(\n",
    "    X_temp, y_time_temp, y_event_temp, test_size=0.176, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Train : {len(X_train)} lignes\")\n",
    "print(f\"üß™ Val   : {len(X_val)} lignes\")\n",
    "print(f\"üîí Test  : {len(X_test)} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Train Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation des matrices\n",
    "\n",
    "def create_aft_inputs(y_time, y_event, X):\n",
    "    y_lower = y_time.values\n",
    "    y_upper = np.where(y_event == 1, y_time.values, np.inf)\n",
    "    return xgb.DMatrix(X, label_lower_bound=y_lower, label_upper_bound=y_upper)\n",
    "\n",
    "# On pr√©pare les donn√©es pour XGBoost\n",
    "dtrain = create_aft_inputs(y_train_time, y_train_event, X_train)\n",
    "dval   = create_aft_inputs(y_val_time, y_val_event, X_val)\n",
    "dtest  = xgb.DMatrix(X_test)\n",
    "\n",
    "print(\"‚úÖ Ingr√©dients pr√™ts : dtrain et dval sont en m√©moire.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "##### Lancement du train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. CONFIGURATION CONNEXION HUGGING FACE\n",
    "# load_dotenv()\n",
    "\n",
    "# mlflow_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "# mlflow_user = os.getenv(\"MLFLOW_TRACKING_USERNAME\")\n",
    "# mlflow_pass = os.getenv(\"MLFLOW_TRACKING_PASSWORD\")\n",
    "\n",
    "# mlflow.set_tracking_uri(mlflow_uri)\n",
    "# os.environ['MLFLOW_TRACKING_USERNAME'] = mlflow_user\n",
    "# os.environ['MLFLOW_TRACKING_PASSWORD'] = mlflow_pass\n",
    "\n",
    "# mlflow.set_experiment(\"Optuna_Zero_Leakage_Search\")\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'objective': 'survival:aft',\n",
    "#         'eval_metric': 'aft-nloglik',\n",
    "#         'tree_method': 'hist',\n",
    "#         'device': 'cuda',\n",
    "#         'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "#         'aft_loss_distribution': trial.suggest_categorical('aft_loss_distribution', ['normal', 'logistic']),\n",
    "#         'aft_loss_distribution_scale': trial.suggest_float('aft_loss_distribution_scale', 0.8, 1.5),\n",
    "#     }\n",
    "\n",
    "#     with mlflow.start_run(nested=True):\n",
    "#         bst = xgb.train(\n",
    "#             params, dtrain, \n",
    "#             num_boost_round=1000,\n",
    "#             evals=[(dval, 'val')],\n",
    "#             early_stopping_rounds=50,\n",
    "#             verbose_eval=False\n",
    "#         )\n",
    "#         preds = bst.predict(dval)\n",
    "#         c_index = concordance_index_censored(y_val_event.astype(bool), y_val_time, -preds)[0]\n",
    "        \n",
    "#         mlflow.log_params(params)\n",
    "#         mlflow.log_metric(\"c_index_val\", c_index)\n",
    "#         return c_index\n",
    "\n",
    "# # Lancement du run Parent\n",
    "# with mlflow.start_run(run_name=\"Optimisation_Intelligente_4060\"):\n",
    "#     study = optuna.create_study(direction='maximize')\n",
    "#     study.optimize(objective, n_trials=50)\n",
    "\n",
    "# print(f\"üî• Termin√© ! Meilleur C-Index : {study.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "##### Train du le meilleur mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. On r√©cup√®re les meilleurs param√®tres\n",
    "# best_params = study.best_params\n",
    "# best_params.update({\n",
    "#     'objective': 'survival:aft',\n",
    "#     'eval_metric': 'aft-nloglik',\n",
    "#     'tree_method': 'hist',\n",
    "#     'device': 'cuda'\n",
    "# })\n",
    "\n",
    "# # 2. On fusionne Train + Val pour un entra√Ænement ultra-solide\n",
    "# # On recr√©√© une DMatrix globale (mais toujours sans le Test !)\n",
    "# X_final_train = pd.concat([X_train, X_val])\n",
    "# y_final_time = pd.concat([y_train_time, y_val_time])\n",
    "# y_final_event = pd.concat([y_train_event, y_val_event])\n",
    "\n",
    "# dtrain_final = create_aft_inputs(y_final_time, y_final_event, X_final_train)\n",
    "\n",
    "# # 3. L'entra√Ænement Champion\n",
    "# print(\"üöÄ Entra√Ænement du mod√®le Champion en cours...\")\n",
    "# final_model = xgb.train(\n",
    "#     best_params,\n",
    "#     dtrain_final,\n",
    "#     num_boost_round=2000,\n",
    "#     verbose_eval=50\n",
    "# )\n",
    "\n",
    "# # 4. LE TEST FINAL (Le coffre-fort)\n",
    "# preds_final_test = final_model.predict(dtest)\n",
    "# c_index_final = concordance_index_censored(y_test_event.astype(bool), y_test_time, -preds_final_test)[0]\n",
    "\n",
    "# print(f\"\\n‚ú® SCORE FINAL SANS FUITE (sur le set de Test) : {c_index_final:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "##### Sauvegarde du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Option 1 : Format JSON (Recommand√© par XGBoost pour la compatibilit√©)\n",
    "# final_model.save_model(\"xgboost_v2.json\")\n",
    "\n",
    "# # Option 2 : Format Pickle (Sauvegarde aussi les param√®tres Python)\n",
    "# import pickle\n",
    "# with open(\"xgboost_v2.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(final_model, f)\n",
    "\n",
    "# print(\"‚úÖ Mod√®le sauvegard√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cr√©e une structure de mod√®le vide\n",
    "final_model = xgb.Booster()\n",
    "\n",
    "# On charge les poids sauvegard√©s\n",
    "final_model.load_model(\"xgboost_v2.json\")\n",
    "\n",
    "print(\"‚úÖ Mod√®le JSON r√©import√© !\")\n",
    "\n",
    "# Test rapide de pr√©diction pour v√©rifier que √ßa tourne\n",
    "# test_preds = final_model_json.predict(dtest)\n",
    "# print(f\"Aper√ßu des pr√©dictions (JSON) : {test_preds[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "##### R√©cup√©ration des gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On r√©cup√®re l'importance bas√©e sur le 'gain' \n",
    "\n",
    "importance = final_model.get_score(importance_type='gain')\n",
    "\n",
    "# Tri et s√©lection des 20 meilleures\n",
    "sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "labels, values = zip(*sorted_importance)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.barh(range(len(labels)), values, color='forestgreen')\n",
    "plt.yticks(range(len(labels)), labels)\n",
    "plt.xlabel('Importance (Gain)')\n",
    "plt.title('Quelles variables pr√©disent la survie ? (Top 20)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Pr√©dictions avec horizon temporel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pr√©dire la dur√©e de vie totale\n",
    "preds_log_vie = final_model.predict(dtest)\n",
    "preds_vie_totale = np.exp(preds_log_vie) \n",
    "\n",
    "# 2. Cr√©ation du tableau de bord\n",
    "\n",
    "df_forecast = pd.DataFrame({\n",
    "    'age_actuel': y_test_time,\n",
    "    'esperance_vie_totale': preds_vie_totale\n",
    "}, index=X_test.index)\n",
    "\n",
    "# 3. Calcul du temps restant avant la fermeture th√©ortique\n",
    "df_forecast['temps_restant_estime'] = df_forecast['esperance_vie_totale'] - df_forecast['age_actuel']\n",
    "\n",
    "# 4. Calcul du risque par horizon\n",
    "\n",
    "df_forecast['ferme_sous_1an'] = df_forecast['temps_restant_estime'] <= 1\n",
    "df_forecast['ferme_sous_2ans'] = df_forecast['temps_restant_estime'] <= 2\n",
    "df_forecast['ferme_sous_3ans'] = df_forecast['temps_restant_estime'] <= 3\n",
    "\n",
    "# Nettoyage : On remplace les valeurs aberrantes (si le mod√®le pr√©dit une mort d√©j√† pass√©e)\n",
    "# par 0 pour le temps restant (alerte imm√©diate)\n",
    "df_forecast['temps_restant_estime'] = df_forecast['temps_restant_estime'].clip(lower=0)\n",
    "\n",
    "print(\"‚úÖ Pr√©visions g√©n√©r√©es avec succ√®s !\")\n",
    "display(df_forecast.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "##### Ajustement des pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. On r√©cup√®re les pr√©dictions brutes (les logs ou les valeurs infinies)\n",
    "preds = final_model.predict(dtest)\n",
    "\n",
    "# 2. On transforme ces pr√©dictions en RANGS (0 √† 1)\n",
    "# Plus la pr√©diction de survie est BASSE, plus le RANG de risque est HAUT\n",
    "# On utilise 'rank' pour ignorer les valeurs \"inf\" et se concentrer sur l'ordre\n",
    "risk_score = pd.Series(preds).rank(pct=True, ascending=False)\n",
    "\n",
    "# 3. Cr√©ation du nouveau tableau de bord\n",
    "df_results = pd.DataFrame({\n",
    "    'age_actuel': y_test_time.values,\n",
    "    'indice_risque': risk_score.values * 100\n",
    "}, index=X_test.index)\n",
    "\n",
    "# 4. Calibration des horizons (Bas√© sur la distribution statistique du risque)\n",
    "# On d√©finit des seuils de criticit√©\n",
    "def evaluer_horizon(score):\n",
    "    if score > 90: return True\n",
    "    return False\n",
    "\n",
    "df_results['alerte_imm√©diate_1an'] = df_results['indice_risque'] > 85\n",
    "df_results['risque_mod√©r√©_2ans'] = df_results['indice_risque'] > 70\n",
    "df_results['risque_3ans'] = df_results['indice_risque'] > 50\n",
    "\n",
    "display(df_results.sort_values('indice_risque', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "##### D√©finition des seuils d'alerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pr√©diction brute du mod√®le\n",
    "preds = final_model.predict(dtest)\n",
    "\n",
    "# 2. Transformation en Indice de Risque (0 √† 100)\n",
    "# On transforme les pr√©dictions en rangs : plus la pr√©diction de survie est basse, \n",
    "# plus l'indice de risque est proche de 100.\n",
    "risk_score = pd.Series(preds).rank(pct=True, ascending=False) * 100\n",
    "\n",
    "# 3. Construction du tableau de bord consolid√©\n",
    "# On r√©cup√®re l'√¢ge actuel et les informations de secteur/taille\n",
    "df_final = pd.DataFrame({\n",
    "    'age_actuel': y_test_time.values,\n",
    "    'indice_risque': risk_score.values\n",
    "}, index=X_test.index)\n",
    "\n",
    "# 4. Attribution des alertes par horizon\n",
    "# Ces seuils sont bas√©s sur la distribution statistique du risque\n",
    "df_final['alerte_1_an'] = df_final['indice_risque'] > 85  # Top 15% plus fragiles\n",
    "df_final['alerte_2_ans'] = df_final['indice_risque'] > 70 # Top 30%\n",
    "df_final['alerte_3_ans'] = df_final['indice_risque'] > 55 # Top 45%\n",
    "\n",
    "# 5. Ajout d'une recommandation textuelle automatique\n",
    "def generer_avis(row):\n",
    "    if row['alerte_1_an']: return \"üî¥ CRITIQUE : Risque de fermeture imminent\"\n",
    "    if row['alerte_2_ans']: return \"üü† VIGILANCE : Fragilit√© √† moyen terme\"\n",
    "    if row['alerte_3_ans']: return \"üü° OBSERVATION : Secteur/Profil instable\"\n",
    "    return \"üü¢ SAIN : Profil de survie solide\"\n",
    "\n",
    "df_final['avis_expert'] = df_final.apply(generer_avis, axis=1)\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(\"‚úÖ Analyse de survie consolid√©e :\")\n",
    "display(df_final.sort_values('indice_risque', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "##### D√©finition des cat√©gories de risques de fermeture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# On cr√©e des cat√©gories de risque\n",
    "df_results['Niveau_Risque'] = pd.cut(df_results['indice_risque'], \n",
    "                                     bins=[0, 50, 75, 90, 100], \n",
    "                                     labels=['Faible', 'Mod√©r√©', '√âlev√©', 'Critique'])\n",
    "\n",
    "fig = px.histogram(df_results, x=\"Niveau_Risque\", \n",
    "                   color=\"Niveau_Risque\",\n",
    "                   title=\"R√©partition du Risque de Fermeture (Portefeuille Test)\",\n",
    "                   color_discrete_map={'Critique':'#8B0000', '√âlev√©':'#FF4500', 'Mod√©r√©':'#FFA500', 'Faible':'#2E8B57'})\n",
    "\n",
    "fig.update_layout(xaxis_title=\"Cat√©gorie de Risque (bas√©e sur l'indice 0-100)\", yaxis_title=\"Nombre d'entreprises\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "##### Visus des secteurs les plus √† risques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fusionne les scores avec les noms de secteurs (APE)\n",
    "top_risques = df_results[df_results['indice_risque'] > 90].join(X_test)\n",
    "sectors_at_risk = top_risques[[col for col in X_test.columns if 'APE_' in col]].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"üö® Secteurs les plus repr√©sent√©s dans le top 10% de risque :\")\n",
    "print(sectors_at_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "##### D√©termination du risque final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. On cr√©e un DataFrame avec les secteurs et les horizons\n",
    "df_strat = df_results.join(X_test)\n",
    "\n",
    "# 2. On groupe par secteur APE pour voir le % de risque √† 1 et 3 ans\n",
    "\n",
    "ape_cols = [col for col in X_test.columns if 'APE_' in col]\n",
    "analyse_secteurs = []\n",
    "\n",
    "for col in ape_cols:\n",
    "    subset = df_strat[df_strat[col] == 1]\n",
    "    if len(subset) > 100: \n",
    "        analyse_secteurs.append({\n",
    "            'Secteur': col.replace('APE_', ''),\n",
    "            'Volume': len(subset),\n",
    "            'Risque_1an_%': subset['alerte_imm√©diate_1an'].mean() * 100,\n",
    "            'Risque_2ans_%': subset['risque_mod√©r√©_2ans'].mean() * 100,\n",
    "            'Risque_3ans_%': subset['risque_3ans'].mean() * 100,\n",
    "        })\n",
    "\n",
    "df_synthese = pd.DataFrame(analyse_secteurs)\n",
    "\n",
    "# On calcule une colonne de tri intelligente : la moyenne pond√©r√©e du risque\n",
    "df_synthese['Fragilit√©_Score'] = (df_synthese['Risque_1an_%'] + df_synthese['Risque_2ans_%'] + df_synthese['Risque_3ans_%']) / 3\n",
    "\n",
    "df_synthese = df_synthese.sort_values('Risque_2ans_%', ascending=False)\n",
    "\n",
    "print(\"üìã SYNTH√àSE STRAT√âGIQUE COMPL√àTE (Horizons 1, 2 et 3 ans)\")\n",
    "display(df_synthese.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "##### Trajectoire du risque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On s√©lectionne le Top 5 des secteurs les plus fragiles (selon Fragilit√©_Score)\n",
    "top_5_fragile = df_synthese.sort_values('Fragilit√©_Score', ascending=False).head(5)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for index, row in top_5_fragile.iterrows():\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=['1 an', '2 ans', '3 ans'],\n",
    "        y=[row['Risque_1an_%'], row['Risque_2ans_%'], row['Risque_3ans_%']],\n",
    "        mode='lines+markers',\n",
    "        name=row['Secteur'],\n",
    "        line=dict(width=3),\n",
    "        marker=dict(size=10)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Trajectoire du risque : √âvolution du % de fermeture pr√©dit\",\n",
    "    xaxis_title=\"Horizon temporel\",\n",
    "    yaxis_title=\"% d'entreprises √† risque\",\n",
    "    hovermode=\"x unified\",\n",
    "    template=\"plotly_white\",\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Pr√©dictions finales sur le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CALCUL DU RISQUE INDIVIDUEL ---\n",
    "preds = final_model.predict(dtest)\n",
    "\n",
    "risk_score = pd.Series(preds).rank(pct=True, ascending=False) * 100\n",
    "\n",
    "# Cr√©ation de la table de travail interne\n",
    "df_work = pd.DataFrame({\n",
    "    'idx': X_test.index,\n",
    "    'risk': risk_score.values,\n",
    "    'a1': (risk_score.values > 85),\n",
    "    'a2': (risk_score.values > 70),\n",
    "    'a3': (risk_score.values > 55)\n",
    "}).set_index('idx').join(X_test)\n",
    "\n",
    "# --- 2. AGR√âGATION PAR SECTEUR (SYNTH√àSE) ---\n",
    "ape_cols = [col for col in X_test.columns if col.startswith('APE_')]\n",
    "synthese_data = []\n",
    "\n",
    "for col in ape_cols:\n",
    "    subset = df_work[df_work[col] == 1]\n",
    "    if len(subset) > 100:\n",
    "        synthese_data.append({\n",
    "            'Secteur': col.replace('APE_', ''),\n",
    "            'Entreprises': len(subset),\n",
    "            'Risque_1an_%': subset['a1'].mean() * 100,\n",
    "            'Risque_2ans_%': subset['a2'].mean() * 100,\n",
    "            'Risque_3ans_%': subset['a3'].mean() * 100\n",
    "        })\n",
    "\n",
    "# --- 3. AFFICHAGE FINAL ---\n",
    "df_synthese = pd.DataFrame(synthese_data)\n",
    "\n",
    "df_synthese['Fragilit√©_Score'] = df_synthese[['Risque_1an_%', 'Risque_2ans_%', 'Risque_3ans_%']].mean(axis=1)\n",
    "\n",
    "print(\"üìã SYNTH√àSE STRAT√âGIQUE DES RISQUES PAR SECTEUR\")\n",
    "display(df_synthese.sort_values('Risque_2ans_%', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "#### Import du mod√®le pour v√©rification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On cr√©e une structure de mod√®le vide\n",
    "# loaded_model_json = xgb.Booster()\n",
    "\n",
    "# # On charge les poids sauvegard√©s\n",
    "# loaded_model_json.load_model(\"champion_model_survie.json\")\n",
    "\n",
    "# print(\"‚úÖ Mod√®le JSON r√©import√© !\")\n",
    "\n",
    "# # Test rapide de pr√©diction pour v√©rifier que √ßa tourne\n",
    "# test_preds = loaded_model_json.predict(dtest)\n",
    "# print(f\"Aper√ßu des pr√©dictions (JSON) : {test_preds[:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-business",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
