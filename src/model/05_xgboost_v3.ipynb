{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954260af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import os\n",
    "import shap\n",
    "import pickle\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from sksurv.metrics import concordance_index_censored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b85b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPU par XGBoost\n",
    "try:\n",
    "    # On cr√©e une micro-matrice de test\n",
    "    data = xgb.DMatrix([[1, 2], [3, 4]], label=[1, 0])\n",
    "\n",
    "    params = {'tree_method': 'gpu_hist', 'device': 'cuda'}\n",
    "    xgb.train(params, data, num_boost_round=1)\n",
    "    print(\"‚úÖ Succ√®s ! La RTX 4060 est reconnue et configur√©e.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå √âchec du GPU : {e}\")\n",
    "    print(\"Le mod√®le tournera sur CPU par d√©faut.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33814b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('dataset_full.parquet')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(f\"Structure du dataset : {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f335cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. G√âOGRAPHIE ---\n",
    "df['dep'] = df[\"Code du d√©partement de l'√©tablissement\"].astype(str).str.zfill(2)\n",
    "\n",
    "# --- 2. TRAITEMENT DE BASE ---\n",
    "df['Cat√©gorie juridique de l\\'unit√© l√©gale'] = df['Cat√©gorie juridique de l\\'unit√© l√©gale'].astype(str)\n",
    "df['Tranche_effectif_num'] = df['Tranche_effectif_num'].fillna(0).astype(float)\n",
    "df['is_ess'] = df['Economie sociale et solidaire unit√© l√©gale'].map({'O': 1, 'N': 0}).fillna(0)\n",
    "\n",
    "# --- 3. NOUVELLE FEATURE PERTINENTE ---\n",
    "# On calcule la taille moyenne par secteur APE\n",
    "secteur_moyennes = df.groupby('libelle_section_ape')['Tranche_effectif_num'].transform('mean')\n",
    "# Est-ce que la bo√Æte est plus costaude que la moyenne de son secteur ?\n",
    "df['taille_relative_secteur'] = df['Tranche_effectif_num'] - secteur_moyennes\n",
    "\n",
    "# --- 4. ENCODAGE ---\n",
    "df_final = pd.get_dummies(\n",
    "    df, \n",
    "    columns=['libelle_section_ape', \"Cat√©gorie juridique de l'unit√© l√©gale\"], \n",
    "    prefix=['APE', 'CJ'],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# --- 5. NETTOYAGE RIGOUREUX ---\n",
    "# On retire tout ce qui n'est pas une feature d'apprentissage\n",
    "cols_to_drop = [\n",
    "    'SIREN', 'Code postal de l\\'√©tablissement', 'Code commune de l\\'√©tablissement',\n",
    "    'D√©nomination de l\\'unit√© l√©gale', 'Activit√© principale de l\\'unit√© l√©gale',\n",
    "    'Date_fermeture_finale', 'latitude', 'longitude', 'code_ape',\n",
    "    'Code du d√©partement de l\\'√©tablissement', 'Code de la r√©gion de l\\'√©tablissement',\n",
    "    'Economie sociale et solidaire unit√© l√©gale', 'dep',\n",
    "    'densite_salariale' # AU CAS O√ô ELLE TRA√éNE ENCORE\n",
    "]\n",
    "\n",
    "# X contient les features, y_time la survie, y_event l'√©tat (ouvert/ferm√©)\n",
    "X = df_final.drop(columns=[c for c in cols_to_drop if c in df_final.columns] + ['fermeture', 'age_estime'])\n",
    "y_time = df_final['age_estime']\n",
    "y_event = df_final['fermeture'].astype(bool)\n",
    "\n",
    "print(f\"‚úÖ Features (X) pr√™tes sans leakage : {X.shape[1]} colonnes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da50b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. NETTOYAGE G√âOGRAPHIQUE\n",
    "df['dep'] = df[\"Code du d√©partement de l'√©tablissement\"].astype(str).str.zfill(2)\n",
    "\n",
    "# 2. TRAITEMENT DES TYPES\n",
    "df[\"Cat√©gorie juridique de l'unit√© l√©gale\"] = df[\"Cat√©gorie juridique de l'unit√© l√©gale\"].astype(str)\n",
    "df['is_ess'] = df['Economie sociale et solidaire unit√© l√©gale'].map({'O': 1, 'N': 0}).fillna(0)\n",
    "\n",
    "# 3. ENCODAGE (One-Hot)\n",
    "df_final = pd.get_dummies(\n",
    "    df, \n",
    "    columns=['libelle_section_ape', \"Cat√©gorie juridique de l'unit√© l√©gale\"], \n",
    "    prefix=['APE', 'CJ'],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# 4. NETTOYAGE DES CAT√âGORIES RARES (< 0.1%)\n",
    "binary_cols = [c for c in df_final.columns if c.startswith('APE_') or c.startswith('CJ_')]\n",
    "frequencies = df_final[binary_cols].mean()\n",
    "rare_cols = frequencies[frequencies < 0.001].index.tolist()\n",
    "df_final = df_final.drop(columns=rare_cols)\n",
    "print(f\"‚úÇÔ∏è {len(rare_cols)} colonnes rares supprim√©es.\")\n",
    "\n",
    "# 5. S√âLECTION FINALE ET PR√âPARATION DES CIBLES\n",
    "\n",
    "cols_a_exclure = [\n",
    "    'SIREN', 'Code postal de l\\'√©tablissement', 'Code commune de l\\'√©tablissement',\n",
    "    'D√©nomination de l\\'unit√© l√©gale', 'Activit√© principale de l\\'unit√© l√©gale',\n",
    "    'Date_fermeture_finale', 'latitude', 'longitude', 'code_ape',\n",
    "    'Code de la r√©gion de l\\'√©tablissement', 'Economie sociale et solidaire unit√© l√©gale',\n",
    "    'age_estime', 'fermeture', \n",
    "    'densite_salariale',        # Contient l'√¢ge (Leakage)\n",
    "    'Tranche_effectif_num',    # Pr√©dit trop bien la fermeture imminente (Leakage)\n",
    "    'taille_relative_secteur'  # D√©riv√© de l'effectif (Leakage)\n",
    "]\n",
    "\n",
    "# On cr√©e X. On garde 'dep' (il n'est pas dans cols_a_exclure ici) pour le split.\n",
    "X = df_final.drop(columns=[c for c in cols_a_exclure if c in df_final.columns])\n",
    "y_time = df_final['age_estime']\n",
    "y_event = df_final['fermeture'].astype(bool)\n",
    "\n",
    "# Filtrage des √¢ges incoh√©rents\n",
    "mask = y_time > 0\n",
    "X = X[mask]\n",
    "y_time = y_time[mask]\n",
    "y_event = y_event[mask]\n",
    "\n",
    "print(f\"‚úÖ Dataset ENFIN honn√™te : {X.shape[0]} lignes, {X.shape[1]} colonnes (incluant 'dep').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SPLIT INITIAL (On s√©pare le Test final)\n",
    "X_temp, X_test, y_time_temp, y_test_time, y_event_temp, y_test_event = train_test_split(\n",
    "    X, y_time, y_event, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# 2. DEUXI√àME SPLIT (Train vs Validation)\n",
    "X_train, X_val, y_train_time, y_val_time, y_train_event, y_val_event = train_test_split(\n",
    "    X_temp, y_time_temp, y_event_temp, test_size=0.176, random_state=42\n",
    ")\n",
    "\n",
    "# 3. CALCUL DU RISQUE D√âPARTEMENTAL\n",
    "dep_risk_map = pd.concat([X_train, y_train_event], axis=1).groupby(\"dep\")[\"fermeture\"].mean()\n",
    "\n",
    "X_train['risque_departemental'] = X_train['dep'].map(dep_risk_map)\n",
    "X_val['risque_departemental'] = X_val['dep'].map(dep_risk_map)\n",
    "X_test['risque_departemental'] = X_test['dep'].map(dep_risk_map)\n",
    "\n",
    "global_mean = y_train_event.mean()\n",
    "X_train['risque_departemental'] = X_train['risque_departemental'].fillna(global_mean)\n",
    "X_val['risque_departemental'] = X_val['risque_departemental'].fillna(global_mean)\n",
    "X_test['risque_departemental'] = X_test['risque_departemental'].fillna(global_mean)\n",
    "\n",
    "# 4. NETTOYAGE FINAL (Modifi√© pour supprimer TOUT le texte g√©nant)\n",
    "\n",
    "cols_a_supprimer = ['dep', \"Code du d√©partement de l'√©tablissement\", \"libelle_section_ape\"]\n",
    "\n",
    "for df_set in [X_train, X_val, X_test]:\n",
    "    # On ne supprime que si la colonne existe\n",
    "    existantes = [c for c in cols_a_supprimer if c in df_set.columns]\n",
    "    df_set.drop(columns=existantes, inplace=True)\n",
    "\n",
    "print(f\"üìä Train : {len(X_train)} lignes | Features: {X_train.shape[1]}\")\n",
    "print(f\"üß™ Val   : {len(X_val)} lignes\")\n",
    "print(f\"üîí Test  : {len(X_test)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf09aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On pr√©pare les fonctions pour les bornes AFT UNIQUEMENT\n",
    "def create_aft_inputs_clean(y_time, y_event, X):\n",
    "    # On s'assure de ne pas envoyer la colonne 'dep' dans le mod√®le\n",
    "    X_clean = X.drop(columns=['dep']) if 'dep' in X.columns else X\n",
    "    \n",
    "    # Bornes de survie\n",
    "    y_lower = y_time.values\n",
    "    y_upper = np.where(y_event == 1, y_time.values, np.inf)\n",
    "    \n",
    "    # Cr√©ation de la matrice\n",
    "    dmat = xgb.DMatrix(X_clean)\n",
    "    \n",
    "    # On injecte les bornes MAIS PAS le base_margin (pour √©viter le leakage)\n",
    "    dmat.set_info(\n",
    "        label_lower_bound=y_lower,\n",
    "        label_upper_bound=y_upper\n",
    "    )\n",
    "    return dmat\n",
    "\n",
    "# 1. On pr√©pare les matrices SANS margin\n",
    "dtrain = create_aft_inputs_clean(y_train_time, y_train_event, X_train)\n",
    "dval   = create_aft_inputs_clean(y_val_time, y_val_event, X_val)\n",
    "\n",
    "# 2. Pour le test (pareil, pas de margin)\n",
    "X_test_clean = X_test.drop(columns=['dep']) if 'dep' in X_test.columns else X_test\n",
    "dtest = xgb.DMatrix(X_test_clean)\n",
    "\n",
    "print(\"‚úÖ Ingr√©dients 3.0 pr√™ts : Le mod√®le va maintenant apprendre √† pr√©dire la survie sans conna√Ætre l'√¢ge √† l'avance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. CONFIGURATION CONNEXION HUGGING FACE\n",
    "# load_dotenv()\n",
    "\n",
    "# mlflow_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "# mlflow_user = os.getenv(\"MLFLOW_TRACKING_USERNAME\")\n",
    "# mlflow_pass = os.getenv(\"MLFLOW_TRACKING_PASSWORD\")\n",
    "\n",
    "# mlflow.set_tracking_uri(mlflow_uri)\n",
    "# os.environ['MLFLOW_TRACKING_USERNAME'] = mlflow_user\n",
    "# os.environ['MLFLOW_TRACKING_PASSWORD'] = mlflow_pass\n",
    "\n",
    "# mlflow.set_experiment(\"Survival_AFT_With_Age_Margin\")\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'objective': 'survival:aft',\n",
    "#         'eval_metric': 'aft-nloglik',\n",
    "#         'tree_method': 'hist',\n",
    "#         'device': 'cuda', # Ta 4060 va travailler dur ici\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "#         'aft_loss_distribution': trial.suggest_categorical('aft_loss_distribution', ['normal', 'logistic']),\n",
    "#         'aft_loss_distribution_scale': trial.suggest_float('aft_loss_distribution_scale', 0.5, 2.0),\n",
    "#     }\n",
    "\n",
    "#     with mlflow.start_run(nested=True):\n",
    "#         # Entra√Ænement\n",
    "#         bst = xgb.train(\n",
    "#             params, \n",
    "#             dtrain, \n",
    "#             num_boost_round=2000,\n",
    "#             evals=[(dval, 'val')],\n",
    "#             early_stopping_rounds=50,\n",
    "#             verbose_eval=False\n",
    "#         )\n",
    "#         preds = bst.predict(dval)\n",
    "#         c_index = concordance_index_censored(\n",
    "#             y_val_event.astype(bool), \n",
    "#             y_val_time, \n",
    "#             -preds\n",
    "#         )[0]\n",
    "        \n",
    "#         # Logs MLflow\n",
    "#         mlflow.log_params(params)\n",
    "#         mlflow.log_metric(\"c_index_val\", c_index)\n",
    "#         mlflow.log_metric(\"best_iteration\", bst.best_iteration)\n",
    "        \n",
    "#         return c_index\n",
    "\n",
    "# # 2. LANCEMENT DE L'OPTIMISATION\n",
    "# with mlflow.start_run(run_name=\"AFT_Search_Final_Features\"):\n",
    "#     study = optuna.create_study(direction='maximize')\n",
    "#     study.optimize(objective, n_trials=50)\n",
    "\n",
    "# print(f\"üèÜ Meilleur C-Index : {study.best_value}\")\n",
    "# print(f\"üìä Meilleurs param√®tres : {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. On entra√Æne le mod√®le final avec les param√®tres du Trial 21\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'objective': 'survival:aft',\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda'\n",
    "})\n",
    "\n",
    "# Entra√Ænement un peu plus long pour stabiliser les scores d'importance\n",
    "final_model = xgb.train(best_params, dtrain, num_boost_round=1000)\n",
    "\n",
    "# 2. Extraction du GAIN (Contribution √† la pr√©cision)\n",
    "importance_gain = final_model.get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': importance_gain.keys(),\n",
    "    'Gain': importance_gain.values()\n",
    "}).sort_values(by='Gain', ascending=False)\n",
    "\n",
    "# 3. Visualisation du Top 15\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=importance_df.head(15), x='Gain', y='Feature', palette='flare')\n",
    "plt.title(\"üèÜ Top 15 des Variables les plus Pr√©dictives (Mod√®le Honn√™te)\")\n",
    "plt.xlabel(\"Importance (Gain moyen par split)\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des scores\n",
    "importance_dict = final_model.get_score(importance_type='gain')\n",
    "\n",
    "# Cr√©ation d'un DataFrame pour un affichage propre\n",
    "df_importance = pd.DataFrame({\n",
    "    'Feature': list(importance_dict.keys()),\n",
    "    'Gain_Score': list(importance_dict.values())\n",
    "})\n",
    "\n",
    "# Tri par importance d√©croissante\n",
    "df_importance = df_importance.sort_values(by='Gain_Score', ascending=False)\n",
    "\n",
    "# Affichage du Top 20\n",
    "print(\"üèÜ TOP 20 DES FEATURES (PAR GAIN) :\")\n",
    "print(df_importance.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Calcul des SHAP values (explication de la direction)\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X_val.drop(columns=['dep']) if 'dep' in X_val.columns else X_val)\n",
    "\n",
    "# 2. Visualisation du Summary Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_val.drop(columns=['dep']) if 'dep' in X_val.columns else X_val, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9810c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generer_palmares(model, X_template):\n",
    "    palmares = []\n",
    "    \n",
    "    # On r√©cup√®re toutes les colonnes qui commencent par APE_\n",
    "    cols_ape = [c for c in X_template.columns if c.startswith('APE_')]\n",
    "    \n",
    "    print(f\"Analyse de {len(cols_ape)} secteurs d'activit√©...\")\n",
    "\n",
    "    for ape in cols_ape:\n",
    "        # 1. On cr√©e un profil moyen (toutes les colonnes √† 0)\n",
    "        profil = pd.DataFrame(0, index=[0], columns=X_template.columns)\n",
    "        \n",
    "        # 2. On active uniquement le secteur APE en cours\n",
    "        profil[ape] = 1\n",
    "        \n",
    "        # 3. On fixe les autres variables sur une valeur neutre (m√©diane)\n",
    "        if 'dep_risk_map' in profil.columns:\n",
    "            profil['dep_risk_map'] = X_template['dep_risk_map'].median()\n",
    "        \n",
    "        # 4. Pr√©diction log-temps\n",
    "        dmat = xgb.DMatrix(profil)\n",
    "        log_life = model.predict(dmat)[0]\n",
    "        \n",
    "        # 5. Conversion en ann√©es\n",
    "        annees = np.exp(log_life)\n",
    "        \n",
    "        palmares.append({\n",
    "            'Secteur': ape.replace('APE_', ''),\n",
    "            'Esperance_Vie_Mediane': round(annees, 2)\n",
    "        })\n",
    "\n",
    "    # Cr√©ation du DataFrame final\n",
    "    df_palmares = pd.DataFrame(palmares).sort_values(by='Esperance_Vie_Mediane', ascending=False)\n",
    "    return df_palmares\n",
    "\n",
    "# --- EX√âCUTION ---\n",
    "# On utilise X_train pour avoir le template des colonnes\n",
    "df_resultat = generer_palmares(final_model, X_train.drop(columns=['dep']) if 'dep' in X_train.columns else X_train)\n",
    "\n",
    "print(\"\\nüíé LES 5 SECTEURS LES PLUS ROBUSTES :\")\n",
    "print(df_resultat.head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è LES 5 SECTEURS LES PLUS FRAGILES :\")\n",
    "print(df_resultat.tail(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3eb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_palmares_robuste(model, X_template):\n",
    "    palmares = []\n",
    "    cols_ape = [c for c in X_template.columns if c.startswith('APE_')]\n",
    "    \n",
    "    # On d√©finit un profil \"moyen\" pour tout le monde\n",
    "    base_profil = pd.DataFrame(0, index=[0], columns=X_template.columns)\n",
    "    if 'dep_risk_map' in base_profil.columns:\n",
    "        base_profil['dep_risk_map'] = X_template['dep_risk_map'].median()\n",
    "\n",
    "    for ape in cols_ape:\n",
    "        profil = base_profil.copy()\n",
    "        profil[ape] = 1\n",
    "        \n",
    "        # On r√©cup√®re le score brut (log-life)\n",
    "        dmat = xgb.DMatrix(profil)\n",
    "        log_life = model.predict(dmat)[0]\n",
    "        \n",
    "        palmares.append({\n",
    "            'Secteur': ape.replace('APE_', ''),\n",
    "            'Score_Survie': log_life  # On garde le score brut pour le tri\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(palmares).sort_values(by='Score_Survie', ascending=False)\n",
    "    \n",
    "    # On borne l'exponentielle pour l'affichage (max 100 ans pour rester r√©aliste)\n",
    "    df['Vie_Estimee_Ans'] = df['Score_Survie'].apply(lambda x: np.exp(min(x, 4.6))) # 4.6 ~ log(100)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Relance le palmar√®s\n",
    "df_resultat = generer_palmares_robuste(final_model, X_train.drop(columns=['dep']) if 'dep' in X_train.columns else X_train)\n",
    "\n",
    "print(\"\\nüèÜ CLASSEMENT DES SECTEURS (Du plus solide au plus fragile) :\")\n",
    "print(df_resultat[['Secteur', 'Score_Survie']].head(10)) # Top 10\n",
    "print(\"...\")\n",
    "print(df_resultat[['Secteur', 'Score_Survie']].tail(10)) # Bottom 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d92557",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb224a",
   "metadata": {},
   "source": [
    "##### Nouvelle tentative de train avec prise en comtpe de l'age pour API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. CHARGEMENT DES VARIABLES D'ENVIRONNEMENT\n",
    "# load_dotenv()\n",
    "\n",
    "# mlflow_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "# mlflow_user = os.getenv(\"MLFLOW_TRACKING_USERNAME\")\n",
    "# mlflow_pass = os.getenv(\"MLFLOW_TRACKING_PASSWORD\")\n",
    "\n",
    "# # Configuration de la connexion distante\n",
    "# mlflow.set_tracking_uri(mlflow_uri)\n",
    "# os.environ['MLFLOW_TRACKING_USERNAME'] = mlflow_user\n",
    "# os.environ['MLFLOW_TRACKING_PASSWORD'] = mlflow_pass\n",
    "\n",
    "# # 2. PR√âPARATION DES DONN√âES (X_api avec l'√¢ge)\n",
    "# # On s'assure de prendre l'√¢ge estim√© et les colonnes d√©j√† encod√©es dans X\n",
    "# X_api = df_final[[c for c in df_final.columns if c in X.columns or c == 'age_estime']].copy()\n",
    "\n",
    "# # Nettoyage automatique des colonnes non-num√©riques (S√©curit√© pour XGBoost)\n",
    "# for col in X_api.columns:\n",
    "#     if X_api[col].dtype == 'object':\n",
    "#         try:\n",
    "#             X_api[col] = pd.to_numeric(X_api[col])\n",
    "#         except ValueError:\n",
    "#             print(f\"Suppression de la colonne incompatible : {col}\")\n",
    "#             X_api.drop(columns=[col], inplace=True)\n",
    "\n",
    "# # Cible : 1 si ferm√©, 0 si ouvert\n",
    "# y_api = (df_final['fermeture'] == 1).astype(int) \n",
    "\n",
    "# # Split Entra√Ænement / Validation\n",
    "# X_train_api, X_val_api, y_train_api, y_val_api = train_test_split(\n",
    "#     X_api, y_api, test_size=0.2, random_state=42, stratify=y_api\n",
    "# )\n",
    "\n",
    "# # 3. ENTRA√éNEMENT ET TRACKING MLFLOW\n",
    "# mlflow.set_experiment(\"Business_Risk_Classifier_API\")\n",
    "\n",
    "# # On active l'autolog pour capturer les courbes d'apprentissage automatiquement\n",
    "# mlflow.xgboost.autolog()\n",
    "\n",
    "# with mlflow.start_run(run_name=\"XGB_Classifier_With_Age\"):\n",
    "#     # D√©finition des param√®tres\n",
    "#     params = {\n",
    "#         \"n_estimators\": 1000,\n",
    "#         \"learning_rate\": 0.05,\n",
    "#         \"max_depth\": 6,\n",
    "#         \"early_stopping_rounds\": 50,\n",
    "#         \"tree_method\": 'hist',\n",
    "#         \"device\": 'cuda', # Utilise ton GPU\n",
    "#         \"eval_metric\": 'auc'\n",
    "#     }\n",
    "    \n",
    "#     # Log des param√®tres manuels (en plus de l'autolog)\n",
    "#     mlflow.log_params(params)\n",
    "\n",
    "#     # Cr√©ation du mod√®le\n",
    "#     api_model = XGBClassifier(**params)\n",
    "\n",
    "#     # Fit\n",
    "#     api_model.fit(\n",
    "#         X_train_api, y_train_api,\n",
    "#         eval_set=[(X_val_api, y_val_api)],\n",
    "#         verbose=100\n",
    "#     )\n",
    "\n",
    "#     # Calcul des pr√©dictions pour le score final\n",
    "#     preds_proba = api_model.predict_proba(X_val_api)[:, 1]\n",
    "#     auc_score = roc_auc_score(y_val_api, preds_proba)\n",
    "    \n",
    "#     # Log de la m√©trique finale\n",
    "#     mlflow.log_metric(\"final_auc_val\", auc_score)\n",
    "    \n",
    "#     # Sauvegarde du mod√®le en tant qu'artefact MLflow\n",
    "#     mlflow.xgboost.log_model(api_model, artifact_path=\"risk_classifier_model\")\n",
    "    \n",
    "#     print(f\"\\n‚úÖ Entra√Ænement termin√© !\")\n",
    "#     print(f\"üöÄ Pr√©cision (AUC) : {auc_score:.4f}\")\n",
    "#     print(f\"üìç Consultable sur : {mlflow_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e20bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdbeac9",
   "metadata": {},
   "source": [
    "#### Test de predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb636a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_business_risk(age, code_ape, dep_risk_score=None, is_ess=0):\n",
    "    \"\"\"\n",
    "    Fonction de pr√©diction align√©e sur les colonnes r√©elles du mod√®le.\n",
    "    \"\"\"\n",
    "    # 1. On cr√©e le DataFrame avec l'exacte liste des colonnes d'entra√Ænement\n",
    "    input_df = pd.DataFrame(0, index=[0], columns=api_model.feature_names_in_)\n",
    "    \n",
    "    # 2. On remplit les variables de base si elles existent dans le mod√®le\n",
    "    if 'age_estime' in input_df.columns:\n",
    "        input_df['age_estime'] = age\n",
    "    if 'is_ess' in input_df.columns:\n",
    "        input_df['is_ess'] = is_ess\n",
    "    if 'dep_risk_map' in input_df.columns and dep_risk_score is not None:\n",
    "        input_df['dep_risk_map'] = dep_risk_score\n",
    "        \n",
    "    # 3. On active le code APE\n",
    "    column_name = f\"APE_{code_ape}\"\n",
    "    if column_name in input_df.columns:\n",
    "        input_df[column_name] = 1\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Attention : Le secteur '{column_name}' n'est pas reconnu par le mod√®le.\")\n",
    "    \n",
    "    # 4. Pr√©diction\n",
    "    proba = api_model.predict_proba(input_df)[0][1]\n",
    "    \n",
    "    return {\n",
    "        \"score_risque\": round(float(proba) * 100, 2),\n",
    "        \"interpretation\": \"Risque √âlev√©\" if proba > 0.5 else \"Risque Mod√©r√©\"\n",
    "    }\n",
    "\n",
    "# --- TEST ---\n",
    "# On enl√®ve taille_relative_secteur du test car le mod√®le ne l'a pas vu\n",
    "test_pme = predict_business_risk(age=2, code_ape=\"Restauration\", dep_risk_score=0.15)\n",
    "print(f\"‚úÖ R√©sultat pour le test : {test_pme}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3405d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "profils = [\n",
    "    {\"nom\": \"Restau Jeune (2 ans)\", \"age\": 2, \"ape\": \"Restauration\"},\n",
    "    {\"nom\": \"Restau Ancien (15 ans)\", \"age\": 15, \"ape\": \"Restauration\"},\n",
    "    {\"nom\": \"√âlectricit√© (5 ans)\", \"age\": 5, \"ape\": \"Production et distribution d'√©lectricit√©, de gaz, de vapeur et d'air conditionn√©\"}\n",
    "]\n",
    "\n",
    "for p in profils:\n",
    "    res = predict_business_risk(age=p['age'], code_ape=p['ape'])\n",
    "    print(f\"PROFIL: {p['nom']} --> Risque: {res['score_risque']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. On pr√©pare les donn√©es (m√™mes colonnes que pour l'entra√Ænement)\n",
    "\n",
    "X_scoring = X_api[api_model.feature_names_in_]\n",
    "\n",
    "# 2. On lance la pr√©diction sur l'ensemble du dataset\n",
    "# predict_proba renvoie [probabilit√© de 0, probabilit√© de 1]\n",
    "# On prend [:, 1] pour avoir la probabilit√© de FERMETURE\n",
    "print(\"üöÄ Calcul des scores de risque en cours...\")\n",
    "df_final['probabilite_fermeture'] = api_model.predict_proba(X_scoring)[:, 1]\n",
    "\n",
    "# 3. Conversion en pourcentage pour plus de lisibilit√©\n",
    "df_final['score_risque_pct'] = (df_final['probabilite_fermeture'] * 100).round(2)\n",
    "\n",
    "# 4. Affichage des entreprises les plus √† risque qui sont encore OUVERTES\n",
    "print(\"\\nüî• Top 10 des entreprises encore ouvertes les plus √† risque :\")\n",
    "entreprises_a_risque = df_final[df_final['fermeture'] == 0].sort_values(by='score_risque_pct', ascending=False)\n",
    "\n",
    "print(entreprises_a_risque[[\"D√©nomination de l'unit√© l√©gale\", 'age_estime', 'score_risque_pct']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7de1f",
   "metadata": {},
   "source": [
    "##### Test de predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8651df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cr√©e des copies pour simuler le futur\n",
    "X_now = df_final[api_model.feature_names_in_].copy()\n",
    "X_plus_1 = X_now.copy()\n",
    "X_plus_2 = X_now.copy()\n",
    "X_plus_3 = X_now.copy()\n",
    "\n",
    "# On simule le vieillissement\n",
    "X_plus_1['age_estime'] += 1\n",
    "X_plus_2['age_estime'] += 2\n",
    "X_plus_3['age_estime'] += 3\n",
    "\n",
    "print(\"‚è≥ Calcul des pr√©dictions temporelles...\")\n",
    "\n",
    "# On calcule les probabilit√©s de FERMETURE pour chaque horizon\n",
    "# Rappel : predict_proba()[:, 1] donne le risque de fermeture\n",
    "df_final['risque_actuel'] = api_model.predict_proba(X_now)[:, 1]\n",
    "df_final['risque_1_an'] = api_model.predict_proba(X_plus_1)[:, 1]\n",
    "df_final['risque_2_ans'] = api_model.predict_proba(X_plus_2)[:, 1]\n",
    "df_final['risque_3_ans'] = api_model.predict_proba(X_plus_3)[:, 1]\n",
    "\n",
    "# Pour avoir la probabilit√© de SURVIE (ce qui est souvent plus parlant)\n",
    "# Probabilit√© de survie = 1 - Risque de fermeture\n",
    "df_final['survie_1_an_pct'] = ((1 - df_final['risque_1_an']) * 100).round(2)\n",
    "df_final['survie_2_ans_pct'] = ((1 - df_final['risque_2_ans']) * 100).round(2)\n",
    "df_final['survie_3_ans_pct'] = ((1 - df_final['risque_3_ans']) * 100).round(2)\n",
    "\n",
    "# Affichage du r√©sultat\n",
    "cols_view = [\"D√©nomination de l'unit√© l√©gale\", 'age_estime', 'survie_1_an_pct', 'survie_2_ans_pct', 'survie_3_ans_pct']\n",
    "print(\"\\nüìÖ Pr√©visions de survie √† 1, 2 et 3 ans :\")\n",
    "print(df_final[cols_view].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[cols_view].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0416f75",
   "metadata": {},
   "source": [
    "#### Modification pour afficher un taux plus coh√©rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daac687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcul du risque de base (aujourd'hui) en probabilit√© (0 √† 1)\n",
    "proba_now = api_model.predict_proba(X_now)[:, 1]\n",
    "\n",
    "# 2. On remplit les colonnes selon la structure attendue\n",
    "df_final['Indice_Risque'] = proba_now.round(6)\n",
    "\n",
    "# On applique la croissance du risque (l'inverse de ta d√©croissance de survie)\n",
    "# Si survie = 95%, alors Risque = 1 - (Survie_Initiale * 0.95)\n",
    "df_final['Prob_1an'] = (1 - ((1 - proba_now) * 0.95)).round(2)\n",
    "df_final['Prob_2ans'] = (1 - ((1 - proba_now) * 0.95 * 0.92)).round(2)\n",
    "df_final['Prob_3ans'] = (1 - ((1 - proba_now) * 0.95 * 0.92 * 0.88)).round(2)\n",
    "\n",
    "# 3. S√©curit√© pour ne pas avoir de probabilit√©s n√©gatives\n",
    "for col in ['Prob_1an', 'Prob_2ans', 'Prob_3ans']:\n",
    "    df_final[col] = df_final[col].clip(lower=0)\n",
    "\n",
    "# 4. Attribution du Statut_Expert (bas√© sur le risque √† 3 ans)\n",
    "def attribuer_statut(p):\n",
    "    if p < 0.10: return 'üü¢ SAIN'\n",
    "    elif p < 0.25: return 'üü° OBSERVATION'\n",
    "    elif p < 0.50: return 'üü† VIGILANCE'\n",
    "    else: return 'üî¥ CRITIQUE'\n",
    "\n",
    "df_final['Statut_Expert'] = df_final['Prob_3ans'].apply(attribuer_statut)\n",
    "\n",
    "# 5. S√©lection et renommage des colonnes pour correspondre au tableau HTML\n",
    "# Note : Assure-toi que les colonnes 'SIREN' et 'D√©nomination' existent dans df_final\n",
    "colonnes_finales = ['SIREN', \"D√©nomination de l'unit√© l√©gale\", 'Indice_Risque', 'Prob_1an', 'Prob_2ans', 'Prob_3ans', 'Statut_Expert']\n",
    "df_tableau = df_final[colonnes_finales].rename(columns={\"D√©nomination de l'unit√© l√©gale\": \"D√©nomination\"})\n",
    "\n",
    "print(df_tableau.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ba5ac",
   "metadata": {},
   "source": [
    "##### Sauvegarde du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb8989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- SAUVEGARDE DU MOD√àLE VERSION V3 ---\n",
    "\n",
    "# # Option 1 : Format JSON (Compatibilit√© XGBoost pure)\n",
    "# final_model.save_model(\"xgboost_v3.json\")\n",
    "\n",
    "# # Option 2 : Format Pickle (Format Python standard)\n",
    "# with open(\"xgboost_v3.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(final_model, f)\n",
    "\n",
    "# print(\"‚úÖ Mod√®le xgboost_v3 sauvegard√© avec succ√®s aux formats .json et .pkl !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-business",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
