{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from sksurv.metrics import concordance_index_censored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "##### Test GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPU par XGBoost\n",
    "try:\n",
    "    # On crÃ©e une micro-matrice de test\n",
    "    data = xgb.DMatrix([[1, 2], [3, 4]], label=[1, 0])\n",
    "\n",
    "    params = {'tree_method': 'gpu_hist', 'device': 'cuda'}\n",
    "    xgb.train(params, data, num_boost_round=1)\n",
    "    print(\"âœ… SuccÃ¨s ! La RTX 4060 est reconnue et configurÃ©e.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ã‰chec du GPU : {e}\")\n",
    "    print(\"Le modÃ¨le tournera sur CPU par dÃ©faut.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('dataset_full.parquet')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(f\"Structure du dataset : {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AJUSTEMENTS POUR LE MODÃˆLE SURVIVAL AFT ---\n",
    "\n",
    "# 1. NETTOYAGE GÃ‰OGRAPHIQUE\n",
    "df['Code du dÃ©partement de l\\'Ã©tablissement'] = df['Code du dÃ©partement de l\\'Ã©tablissement'].astype(str).str.zfill(2)\n",
    "dep_risk_map = df.groupby(\"Code du dÃ©partement de l'Ã©tablissement\")[\"fermeture\"].mean()\n",
    "df['risque_departemental'] = df['Code du dÃ©partement de l\\'Ã©tablissement'].map(dep_risk_map)\n",
    "\n",
    "# 2. TRAITEMENT DES TYPES\n",
    "df['CatÃ©gorie juridique de l\\'unitÃ© lÃ©gale'] = df['CatÃ©gorie juridique de l\\'unitÃ© lÃ©gale'].astype(str)\n",
    "df['age_estime'] = df['age_estime'].astype(float)\n",
    "df['Tranche_effectif_num'] = df['Tranche_effectif_num'].fillna(0).astype(float)\n",
    "\n",
    "# 3. ENCODAGE DES VARIABLES\n",
    "\n",
    "df['is_ess'] = df['Economie sociale et solidaire unitÃ© lÃ©gale'].map({'O': 1, 'N': 0}).fillna(0).astype(int)\n",
    "\n",
    "# B. One-Hot Encoding\n",
    "\n",
    "df_final = pd.get_dummies(\n",
    "    df, \n",
    "    columns=['libelle_section_ape', 'CatÃ©gorie juridique de l\\'unitÃ© lÃ©gale'], \n",
    "    prefix=['APE', 'CJ'],\n",
    "    drop_first=True,\n",
    "    dtype=int \n",
    ")\n",
    "\n",
    "# 4. SÃ‰LECTION FINALE\n",
    "\n",
    "cols_to_drop = [\n",
    "    'Code postal de l\\'Ã©tablissement', 'Code commune de l\\'Ã©tablissement',\n",
    "    'ActivitÃ© principale de l\\'unitÃ© lÃ©gale', 'Date_fermeture_finale', \n",
    "    'latitude', 'longitude', 'code_ape',\n",
    "    'Code du dÃ©partement de l\\'Ã©tablissement', 'Code de la rÃ©gion de l\\'Ã©tablissement',\n",
    "    'Economie sociale et solidaire unitÃ© lÃ©gale'\n",
    "]\n",
    "df_final = df_final.drop(columns=[c for c in cols_to_drop if c in df_final.columns])\n",
    "\n",
    "# 5. DERNIERS RÃ‰GLAGES POUR LA SURVIE (Crucial pour AFT)\n",
    "\n",
    "df_final = df_final[df_final['age_estime'] > 0].copy()\n",
    "\n",
    "# On crÃ©e les colonnes cibles pour le modÃ¨le AFT\n",
    "\n",
    "df_final['y_lower'] = df_final['age_estime']\n",
    "df_final['y_upper'] = np.where(df_final['fermeture'] == 1, df_final['age_estime'], np.inf)\n",
    "\n",
    "print(f\"âœ… Dataset finalisÃ© : {df_final.shape[0]} lignes, {df_final.shape[1]} colonnes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. ANALYSE DES FRÃ‰QUENCES ---\n",
    "binary_cols = [c for c in df_final.columns if c.startswith('APE_') or c.startswith('CJ_')]\n",
    "frequencies = df_final[binary_cols].mean().sort_values(ascending=False) * 100\n",
    "\n",
    "# DÃ©finition du seuil (0.1%)\n",
    "rare_limit = 0.1\n",
    "rare_cols = frequencies[frequencies < rare_limit]\n",
    "\n",
    "print(f\"--- ðŸ” Analyse des colonnes rares (< {rare_limit}%) ---\")\n",
    "print(f\"Il y a {len(rare_cols)} colonnes concernÃ©es.\")\n",
    "\n",
    "# --- 2. FUSION DES CATÃ‰GORIES RARES ---\n",
    "\n",
    "rare_ape_cols = [c for c in rare_cols.index if c.startswith('APE_')]\n",
    "rare_cj_cols = [c for c in rare_cols.index if c.startswith('CJ_')]\n",
    "\n",
    "# On crÃ©e la colonne \"Autres\" et on supprime les anciennes\n",
    "if rare_ape_cols:\n",
    "    df_final['APE_Autres_Secteurs'] = df_final[rare_ape_cols].any(axis=1).astype(int)\n",
    "    df_final.drop(columns=rare_ape_cols, inplace=True)\n",
    "\n",
    "if rare_cj_cols:\n",
    "    df_final['CJ_Autres_Status'] = df_final[rare_cj_cols].any(axis=1).astype(int)\n",
    "    df_final.drop(columns=rare_cj_cols, inplace=True)\n",
    "\n",
    "# --- 3. PRÃ‰PARATION DES CIBLES DE SURVIE (AFT) ---\n",
    "\n",
    "df_final['y_lower'] = df_final['age_estime']\n",
    "df_final['y_upper'] = np.where(df_final['fermeture'] == 1, df_final['age_estime'], np.inf)\n",
    "\n",
    "print(f\"âœ… Nettoyage et prÃ©paration AFT terminÃ©s.\")\n",
    "print(f\"ðŸ“Š Nouveau nombre de colonnes : {len(df_final.columns)}\")\n",
    "display(df_final[['age_estime', 'fermeture', 'y_lower', 'y_upper']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "##### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. On s'assure d'avoir la colonne d'Ã¢ge\n",
    "if 'age_au_diagnostic' not in df_final.columns:\n",
    "    df_final['age_au_diagnostic'] = df_final['age_estime']\n",
    "\n",
    "# 2. DÃ©finition des colonnes Ã  exclure (on garde l'Ã¢ge !)\n",
    "# Assure-toi que 'risque_departemental' est BIEN DANS to_drop si tu veux l'enlever\n",
    "non_numeric_cols = df_final.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "targets = ['fermeture', 'age_estime', 'y_lower', 'y_upper']\n",
    "to_drop = list(set(non_numeric_cols + targets))\n",
    "\n",
    "if 'age_au_diagnostic' in to_drop:\n",
    "    to_drop.remove('age_au_diagnostic')\n",
    "\n",
    "# --- OPTIONNEL : Si tu veux enlever le risque dÃ©partemental pour rÃ©Ã©quilibrer ---\n",
    "\n",
    "X = df_final.drop(columns=to_drop)\n",
    "y_time = df_final['age_estime']\n",
    "y_event = df_final['fermeture'].astype(int)\n",
    "\n",
    "# 3. Split Train/Test\n",
    "X_train, X_test, y_train_time, y_test_time, y_train_event, y_test_event = train_test_split(\n",
    "    X, y_time, y_event, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4. CrÃ©ation des DMatrix (Le cerveau d'XGBoost)\n",
    "# Pour le train\n",
    "y_upper_train = np.where(y_train_event == 1, y_train_time, np.inf)\n",
    "dtrain = xgb.DMatrix(X_train)\n",
    "dtrain.set_float_info('label_lower_bound', y_train_time.values)\n",
    "dtrain.set_float_info('label_upper_bound', y_upper_train)\n",
    "\n",
    "# Pour le test (Optuna en a besoin pour Ã©valuer)\n",
    "y_upper_test = np.where(y_test_event == 1, y_test_time, np.inf)\n",
    "dtest_optuna = xgb.DMatrix(X_test)\n",
    "dtest_optuna.set_float_info('label_lower_bound', y_test_time.values)\n",
    "dtest_optuna.set_float_info('label_upper_bound', y_upper_test)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es prÃªtes ! Features utilisÃ©es : {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "##### Train dans Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 1. GESTION DE L'EXPERIMENT MLFLOW ---\n",
    "\n",
    "# experiment_name = \"XGBoost_Survival_V3_Equilibre\"\n",
    "\n",
    "# try:\n",
    "\n",
    "#     exp_id = mlflow.create_experiment(experiment_name)\n",
    "# except:\n",
    "\n",
    "#     exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "# mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# # --- 2. DÃ‰FINITION DE L'OBJECTIF ---\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'objective': 'survival:aft',\n",
    "#         'eval_metric': 'aft-nloglik',\n",
    "#         'tree_method': 'hist',\n",
    "#         'device': 'cuda',\n",
    "        \n",
    "#         # --- RÃ‰GLAGES POUR LA NUANCE (Anti-Binaire) ---\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 6), \n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.02, log=True),\n",
    "        \n",
    "#         # Force des feuilles plus denses pour Ã©viter le 0% de risque brutal\n",
    "#         'min_child_weight': trial.suggest_int('min_child_weight', 10, 50),\n",
    "        \n",
    "#         # RÃ©gularisation forte pour contrer la domination du dÃ©partement\n",
    "#         'lambda': trial.suggest_float('lambda', 1.0, 50.0, log=True),\n",
    "#         'alpha': trial.suggest_float('alpha', 0.1, 10.0, log=True),\n",
    "        \n",
    "#         'aft_loss_distribution': 'logistic', \n",
    "#         'aft_loss_distribution_scale': trial.suggest_float('aft_loss_distribution_scale', 0.8, 1.5),\n",
    "        \n",
    "#         # On rÃ©duit le sampling pour que chaque arbre voit des sous-ensembles diffÃ©rents\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "#     }\n",
    "\n",
    "#     # Un run \"nested\" (imbriquÃ©) pour chaque essai Optuna\n",
    "#     with mlflow.start_run(nested=True):\n",
    "#         bst_trial = xgb.train(\n",
    "#             params, \n",
    "#             dtrain, \n",
    "#             num_boost_round=2000, \n",
    "#             evals=[(dtest_optuna, 'test')],\n",
    "#             early_stopping_rounds=100,\n",
    "#             verbose_eval=False\n",
    "#         )\n",
    "        \n",
    "#         score = bst_trial.best_score\n",
    "#         mlflow.log_params(params)\n",
    "#         mlflow.log_metric(\"test_nloglik\", score)\n",
    "        \n",
    "#         return score\n",
    "\n",
    "# # --- 3. LANCEMENT DE L'OPTIMISATION ---\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# # On englobe l'Ã©tude dans un run parent identifiÃ© par l'exp_id\n",
    "# with mlflow.start_run(run_name=\"Train_V3_Equilibre_Anciennete\", experiment_id=exp_id):\n",
    "#     study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # --- 4. RÃ‰CUPÃ‰RATION DU MEILLEUR MODÃˆLE ---\n",
    "# print(f\"âœ… Meilleur score : {study.best_value}\")\n",
    "# print(f\"ðŸ† ParamÃ¨tres : {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "##### Sauvegarde du modÃ¨le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. RÃ©cupÃ©ration des meilleurs paramÃ¨tres\n",
    "# best_params = study.best_params\n",
    "# best_params.update({\n",
    "#     'objective': 'survival:aft',\n",
    "#     'eval_metric': 'aft-nloglik',\n",
    "#     'tree_method': 'hist',\n",
    "#     'device': 'cuda',\n",
    "#     'aft_loss_distribution': 'logistic'\n",
    "# })\n",
    "\n",
    "# print(\"ðŸš€ RÃ©-entraÃ®nement du modÃ¨le final avec les meilleurs paramÃ¨tres...\")\n",
    "\n",
    "# # 2. EntraÃ®nement final (on utilise dtrain qui contient 100% du train)\n",
    "# final_model = xgb.train(\n",
    "#     best_params,\n",
    "#     dtrain,\n",
    "#     num_boost_round=2000 # On peut monter un peu plus pour la version finale\n",
    "# )\n",
    "\n",
    "# # 3. Sauvegarde au format JSON (plus sÃ»r pour ton API)\n",
    "# final_model.save_model(\"model_survie_V3_final.json\")\n",
    "# print(\"âœ… ModÃ¨le V3 sauvegardÃ© sous 'model_survie_V3_final.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "##### VÃ©rification des features importances sur les donnÃ©es de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. PERFORMANCE SUR LE TEST SET ---\n",
    "# On calcule le score final sur les donnÃ©es de test non vues\n",
    "eval_result = final_model.eval(dtest_optuna)\n",
    "print(f\"ðŸ“Š Performance finale (Test NLogLik) : {eval_result}\")\n",
    "\n",
    "# --- 2. IMPORTANCE DES VARIABLES ---\n",
    "# On regarde ce qui a vraiment comptÃ© pour le modÃ¨le\n",
    "importance = final_model.get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': importance.keys(),\n",
    "    'Importance': importance.values()\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ† Top 10 des variables les plus influentes :\")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Test de prÃ©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. PRÃ‰DICTION ET RÃ‰PARTITION ---\n",
    "\n",
    "preds_mu = final_model.predict(dtest_optuna)\n",
    "sigma = best_params['aft_loss_distribution_scale']\n",
    "\n",
    "# On calcule un risque arbitraire Ã  2 ans\n",
    "def calculate_risk(mu, horizon=2, s=sigma):\n",
    "    z = (np.log(horizon) - mu) / s\n",
    "    return (1 / (1 + np.exp(-z))) * 100\n",
    "\n",
    "# Application aux rÃ©sultats\n",
    "risques_2ans = [calculate_risk(m) for m in preds_mu]\n",
    "\n",
    "# CrÃ©ation d'un mini-df d'analyse\n",
    "analysis_df = pd.DataFrame({\n",
    "    'mu': preds_mu,\n",
    "    'risk_2y': risques_2ans,\n",
    "    'age_reel': X_test['age_au_diagnostic']\n",
    "})\n",
    "\n",
    "# DÃ©finition des profils (ex: > 50% de risque Ã  2 ans = Critique)\n",
    "critique = analysis_df[analysis_df['risk_2y'] > 50]\n",
    "solide = analysis_df[analysis_df['risk_2y'] <= 10]\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"ðŸ¢ Entreprises analysÃ©es (Test) : {len(analysis_df)}\")\n",
    "print(f\"ðŸ”´ Portefeuille critique (>50% risque) : {len(critique)} ({len(critique)/len(analysis_df):.1%})\")\n",
    "print(f\"ðŸŸ¢ Portefeuille solide (<10% risque) : {len(solide)} ({len(solide)/len(analysis_df):.1%})\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"ðŸ’¡ Ã‚ge moyen (Profil Critique) : {critique['age_reel'].mean():.1f} ans\")\n",
    "print(f\"ðŸ‘´ Ã‚ge moyen (Profil Solide) : {solide['age_reel'].mean():.1f} ans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_risk_safe(mu, horizon=5, s=sigma):\n",
    "    # On utilise np.clip pour Ã©viter que z ne devienne trop grand (overflow)\n",
    "    z = (np.log(horizon) - mu) / s\n",
    "    z = np.clip(z, -50, 50) \n",
    "    # Logistique robuste\n",
    "    return (1 / (1 + np.exp(-z))) * 100\n",
    "\n",
    "# On teste sur un horizon plus lointain (5 ans) pour voir les fragilitÃ©s\n",
    "analysis_df['risk_5y'] = analysis_df['mu'].apply(lambda x: calculate_risk_safe(x, horizon=5))\n",
    "\n",
    "# On baisse un peu le seuil du \"critique\" pour le diagnostic (ex: 20% Ã  5 ans)\n",
    "critique = analysis_df[analysis_df['risk_5y'] > 20]\n",
    "solide = analysis_df[analysis_df['risk_5y'] <= 5]\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"ðŸ¢ Entreprises analysÃ©es : {len(analysis_df)}\")\n",
    "print(f\"ðŸ”´ Profil Fragile (>20% risque Ã  5 ans) : {len(critique)} ({len(critique)/len(analysis_df):.1%})\")\n",
    "print(f\"ðŸŸ¢ Profil TrÃ¨s Solide (<5% risque Ã  5 ans) : {len(solide)} ({len(solide)/len(analysis_df):.1%})\")\n",
    "print(\"-\" * 50)\n",
    "if len(critique) > 0:\n",
    "    print(f\"ðŸ’¡ Ã‚ge moyen (Fragile) : {critique['age_reel'].mean():.1f} ans\")\n",
    "print(f\"ðŸ‘´ Ã‚ge moyen (Solide) : {solide['age_reel'].mean():.1f} ans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "##### VÃ©rification des risques du portefeuille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PrÃ©paration de la matrice globale (X_total)\n",
    "\n",
    "X_total = df_final[X.columns] \n",
    "dmatrix_total = xgb.DMatrix(X_total)\n",
    "\n",
    "# 2. PrÃ©diction des Mu et calcul des risques\n",
    "preds_mu_total = final_model.predict(dmatrix_total)\n",
    "sigma = best_params['aft_loss_distribution_scale']\n",
    "\n",
    "def get_risk(mu, horizon=5):\n",
    "    z = (np.log(horizon) - mu) / sigma\n",
    "    z = np.clip(z, -50, 50)\n",
    "    return (1 / (1 + np.exp(-z))) * 100\n",
    "\n",
    "# 3. CrÃ©ation du DataFrame de rÃ©partition\n",
    "ptf_analysis = pd.DataFrame({\n",
    "    'age': df_final['age_au_diagnostic'],\n",
    "    'risk_5y': [get_risk(m) for m in preds_mu_total]\n",
    "})\n",
    "\n",
    "# 4. Segmentation en Ã©chelons (Labels de risque)\n",
    "bins = [0, 5, 10, 20, 100]\n",
    "labels = ['ðŸŸ¢ TrÃ¨s Solide', 'ðŸŸ¡ Stable', 'ðŸŸ  Fragile', 'ðŸ”´ Critique']\n",
    "ptf_analysis['segment'] = pd.cut(ptf_analysis['risk_5y'], bins=bins, labels=labels)\n",
    "\n",
    "# 5. Calcul de la rÃ©partition\n",
    "repartition = ptf_analysis.groupby('segment', observed=True).agg(\n",
    "    nombre_entreprises=('age', 'count'),\n",
    "    age_moyen=('age', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "repartition['pourcentage'] = (repartition['nombre_entreprises'] / len(ptf_analysis) * 100).round(2)\n",
    "\n",
    "print(\"ðŸ“Š RÃ‰PARTITION GLOBALE DU PORTEFEUILLE (Horizon 5 ans)\")\n",
    "print(\"-\" * 60)\n",
    "print(repartition.to_string(index=False))\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "##### VÃ©rification des secteurs les plus Ã  risques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 des secteurs les plus reprÃ©sentÃ©s dans le segment Critique\n",
    "critique_df = df_final.loc[ptf_analysis[ptf_analysis['segment'] == 'ðŸ”´ Critique'].index]\n",
    "top_ape_critique = critique_df.filter(like='APE_').sum().sort_values(ascending=False).head(5)\n",
    "\n",
    "print(\"ðŸ”¥ Top 5 des secteurs les plus Ã  risque (Segment Critique) :\")\n",
    "print(top_ape_critique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-business",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
